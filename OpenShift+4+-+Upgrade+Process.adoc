The following documentation is provided in the https://confluence.capgroup.com/pages/viewpage.action?spaceKey=CNTEN&title=Openshift+4+-+Upgrade+Process[Capital Group Confluence].

OpenShift 4 - Upgrade Process
•	Upgrade Process
•	Resolving Common Errors
•	Required Checks Before/After Upgrades
Upgrade Process
The example below shows an upgrade from OpenShift 4.3 to 4.5.
1.	For On-Prem clusters, you must upgrade Portworx to the supported version before starting the OpenShift cluster upgrade. Please follow up with the Portworx support engineering team to identify the targeted Portworx version. Once the Portworx version has been identified, follow the steps in this document to upgrade the Porxworx version. https://docs.portworx.com/portworx-install-with-kubernetes/operate-and-maintain-on-kubernetes/upgrade/
2.	Log into the Web console for the cluster and go to Administration - Cluster Settings. Make sure that the cluster is healthy. You must finish the minor version upgrade before doing the major version upgrade.  
3.	Identify the Upgrade Path in OpenShift 4 
1.	The <CHANNEL> query parameter needs to be specified based on the OpenShift cluster's channel. 
The <ARCH> query parameter needs to be the same as the cluster's architecture. The amd64 arch is used to indicate the x86_64 architecture.
2.	curl -H "Accept: application/json" https://api.openshift.com/api/upgrades_info/v1/graph?channel=<CHANNEL>&arch=<ARCH> | jq '.'
4.	A dot graph can be generated which may make it easier to understand the available upgrade paths. 
1.	Download the graph.sh file https://github.com/openshift/cincinnati/blob/master/hack/graph.sh
2.	curl -sH 'Accept:application/json' 'https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.3&arch=amd64' | ./graph.sh | dot -Tsvg > graph.svg
 
5.	Upgrade to the next minor version
1.	From the web console, click Administration > Cluster Settings and review the contents of the Overview tab.
2.	For production clusters, ensure that the CHANNEL is set to stable-4.5.
3.	If the UPDATE STATUS is not Updates Available, you cannot upgrade your cluster.
4.	The DESIRED VERSION indicates the cluster version that your cluster is running or is updating to.
5.	Click Updates Available, select a version to update to, and click Update.
6.	The UPDATE STATUS changes to Updating, and you can review the progress of the Operator upgrades on the Cluster Operators tab.
6.	 

7.	If the "Update now" button is not available, the Failing status will show. Click on the "View details" link to see the failure.
 
8.	Look for operators that are in degraded status. Click on the operator name to see details about the error. Do not proceed with the update until the errors are resolved.
9.	In this example, you can see that monitoring is degraded
 

Go to Workloads > Pods, and enter openshift-monitoring as the project name. Fix the errors in the pods.
 
10.	Once all errors have been resolved, go back to Administration - Cluster Settings, switch to the Cluster Operators tab and make sure that all operators have the same version number. This should be the case if none of them are in states Updating or Degraded.
11.	OpenShift marks each worker node as unavailable during the update. This can cause the MachineAutoscaler to add new worker nodes with the old version and delete worker nodes with the new version during the update, unnecessarily slowing down the update. To avoid this, temporarily save and delete all MachineAutoscalers. 

To save them, connect to the OpenShift cluster's control host via SSH, authenticate to the cluster with the "oc" CLI and run the following command:
oc get machineautoscaler -n openshift-machine-api -o yaml >machineautoscalers.yaml
Do not proceed if this command prints any error message.

If there are no error messages, proceed with deleting the MachineAutoscalers:
oc delete machineautoscaler -n openshift-machine-api --all
12.	Remove twistlock daemonset on the cluster before you upgrade.  There are known issues with twistlock defenders that causes worker nodes to go out of sync, so let's delete to avoid this during upgrade.
13.	oc project twistlock
14.	
15.	oc get ds
16.	NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
twistlock-defender-ds   7         7         6       7            6           <none>          48d
17.	On the bastion server, change directory to twistlock-20-24 and run an 'oc delete -f <yaml_file> to remove the daemonset
18.	cd /home/ec2-user/twistlock-20-04
19.	
20.	ls
21.	ocp4-int-dev-twistlock-20-04-defender.yaml
22.	
23.	#This will delete the daemonset from step #7. 
oc delete -f ocp4-int-dev-twistlock-20-04-defender.yaml
24.	Once the above requirements have been met, update the OpenShift cluster as documented in https://docs.openshift.com/container-platform/4.4/updating/updating-cluster-between-minor.html. During the update, the nodes will automatically reboot in a rolling fashion.
25.	After the update has finished, the page Administration - Cluster Settings should show the new version in the Current Version field. The Cluster Operators tab should show that all operators are updated to the new version, and that all of them are available (not degraded or updating).
26.	Go to Compute - Machine Config Pools and verify that both the master and the worker pool show Updated: True, Updating: False, Degraded: False

It will take some time for all nodes to be updated, even if the Cluster Settings page shows that the cluster has already been updated. To check the progress, click on one of the machine config pools. It will show how many nodes / machines have been updated already.

If the number does not change for a long time (>10 minutes), check on the Details and the Events tab of the machine config pool for messages like "on-disk file does not match expected state".

In our experience, this happens because Twistlock overwrites /etc/crio/crio.conf with its own version and deleted /etc/rhsm/rhsm.conf. To manually fix this issue, you need to copy the crio.conf from the master node to the work node. Then you need to wait for the operator to pick up the new change. To restore the version that OpenShift expects, run the Ansible playbook restore-crio-conf.yaml and restor-rhsm-conf.yaml as described in the README.
27.	Restore the MachineAutoscalers with the following command. This has to be run in the same directory where the "oc get machineautoscaler ..." command was run.
oc create -f machineautoscalers.yaml
If successful, the command will show the name of each MachineAutoscaler that was created.
28.	Restore the twistlock daemonset from the yaml file
oc create -f ocp4-int-dev-twistlock-20-04-defender.yaml
Resolving Common Errors
Error	Observation	Resolution
DaemonSet "openshift-multus/multus" is not available (awaiting 1 nodes)
DaemonSet "openshift-sdn/ovs" is not available (awaiting 1 nodes)
DaemonSet "openshift-sdn/sdn" is not available (awaiting 1 nodes)
	The worker node seems to be not coming after deleting	Delete the bad node. It will take a few minutes after the bad node was deleted for the system to get back to normal.


level=error ts=2020-05-08T21:39:06.351Z caller=coordinator.go:124 component=configuration msg="Loading configuration file failed" file=/etc/alertmanager/config/alertmanager.yaml err="open /etc/alertmanager/config/alertmanager.yaml: no such file or directory"
2:47
Failed to rollout the stack. Error: running task Updating Alertmanager failed: waiting for Alertmanager object changes failed: waiting for Alertmanager: expected 3 replicas, updated 0 and available 0	 	Fixed the alert manager yaml typo resolved the issue. 
Unable to apply 4.3.18: the cluster operator ingress is degraded	This issue is related to Route 53 configuration in AWS. Multiple routes were defined with the same tag values. OpenShift will look for the first tag and get the corresponding route and IP address. That was causing the problem.

 	Removed the unused routes and tags in Route 53 fixed this issue.
cluster operator monitoring degraded with message: 
Failed to rollout the stack. Error: the Cluster Monitoring ConfigMap doesn't contain a 'config.yaml' key	This can happen if somebody manually created the ConfigMap cluster-monitoring-config in namespace openshift-monitoring but didn't include the "config.yaml" key in it. OpenShift versions before 4.4 ignored this. In OpenShift 4.4, the monitoring operator fails if the "config.yaml" key is missing.	Delete the ConfigMap cluster-monitoring-config in namespace openshift-monitoring after saving its YAML content as a backup.
cluster operator csi-snapshot-controller degraded with message:
failed to sync CRDs: CustomResourceDefinition.apiextensions.k8s.io "volumesnapshots.snapshot.storage.k8s.io" is invalid: status.storedVersions[0]: Invalid value: "v1alpha1": must appear in spec.versions	This can happen if an older version of the custom resource definition was previously installed in the OpenShift cluster. This is a known problem with some OpenShift 4.4 upgrades.	A workaround is described in support ticket 02652529 linked from https://access.redhat.com/solutions/5069531
Run the following command to delete the CRDs that cause this problem.
oc delete crd \
  volumesnapshotclasses.snapshot.storage.k8s.io \
  volumesnapshotcontents.snapshot.storage.k8s.io \
  volumesnapshots.snapshot.storage.k8s.io
The operator will recreate them with the right content.
Required Checks Before/After Upgrades
Regarding the 4.3 -> 4.4 -> 4.5 upgrade process, before and after each upgrade, you must check that the cluster is in a healthy state:
Run the following playbook for validation.  
https://bitbucket.capgroup.com/projects/PLAT/repos/cloud_automation/browse/AWS/compute/ansible/playbooks/ocp_post_upgrade_checks.yaml
•	oc get nodes to make sure that all nodes are the same version
•	oc get co to make sure that the cluster operators are all running
•	oc get clusterversion is happy
•	oc get mcp # keep note of # of worker and master nodes. Confirm nothing is "updating" when you start the upgrade. Make sure nothing is "degraded" before the upgrade.
•	oc get ds -A showing all daemonset pods running
•	oc get pod -A # status of all PODs
•	oc get pod -A | grep -v Running | grep -v Completed # showing all erroring pods
•	oc get event -A --sort-by metadata.creationTimestamp showing no errors
•	No Prometheus alerts firing
If the cluster seems healthy, then go ahead and start the next upgrade. If you can do multiple upgrades within the same maintenance window that’s great.
If you see worker nodes in different version after the upgrade, you need to follow these steps.
1. Run the 'oc get nodes' command to identify nodes in different versions
$ oc get nodes
compute-0         Ready                      worker   274d   v1.14.6-152-g117ba1f
compute-1         Ready                      worker   274d   v1.14.6-152-g117ba1f
compute-2         Ready                      worker   274d   v1.14.6-152-g117ba1f
compute-3         Ready,SchedulingDisabled   worker   244d   v1.14.6+0a21dd3b3
control-plane-0   Ready                      master   274d   v1.16.2
control-plane-1   Ready                      master   274d   v1.16.2
control-plane-2   Ready                      master   225d   v1.16.2

2. Check if the machine pools are in degraded states. 
$ oc get mcp
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECO
master   rendered-master-25be7f6fd1d6ec4cee0ace77a2c64856   True      False      False      3              3                   3                     0
worker   rendered-worker-4b5261aab73d037696d8e2f576847e77   False     True       True       4              0                   0                     1
3. Check the cluster operator status for issues.
$ oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.3.18    True        False         False      89d
cloud-credential                           4.3.18    True        False         False      271d
cluster-autoscaler                         4.3.18    True        False         False      271d
console                                    4.3.18    True        False         False      7h26m
dns                                        4.3.18    True        False         False      69d
image-registry                             4.3.18    True        False         False      25d
ingress                                    4.3.18    True        False         False      69d
insights                                   4.3.18    True        False         False      89d
kube-apiserver                             4.3.18    True        False         False      271d
kube-controller-manager                    4.3.18    True        False         False      271d
kube-scheduler                             4.3.18    True        False         False      271d
machine-api                                4.3.18    True        False         False      271d
machine-config                             4.3.18    True        False         False      20d
marketplace                                4.3.18    True        False         False      7h33m
monitoring                                 4.3.18    True        False         False      87m
network                                    4.3.18    True        False         False      271d
node-tuning                                4.3.18    True        False         False      7h34m
openshift-apiserver                        4.3.18    True        False         False      7h25m
openshift-controller-manager               4.3.18    True        False         False      69d
openshift-samples                          4.3.18    True        False         False      7h57m
operator-lifecycle-manager                 4.3.18    True        False         False      271d
operator-lifecycle-manager-catalog         4.3.18    True        False         False      271d
operator-lifecycle-manager-packageserver   4.3.18    True        False         False      7h27m
service-ca                                 4.3.18    True        False         False      271d
service-catalog-apiserver                  4.3.18    True        False         False      271d
service-catalog-controller-manager         4.3.18    True        False         False      271d
storage                                    4.3.18    True        False         False      8h
4. Check the machine-config-daemon-host.service logs.
$ oc debug node/<worker node>
$ chroot /host
$ journalctl -u machine-config-daemon-host.service --no-pager


