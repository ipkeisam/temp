The following documentation is provided at the https://confluence.capgroup.com/display/CNTEN/OCP+4.2+Cluster+Provisioning+AWS[Capital Group Confluence].

OCP 4.2 Cluster Provisioning AWS
If you are installing Openshift 4 on a new AWS account, some pre-reqs will need to be completed
	Firewall Request for VPC
	Link AWS account to Red Hat for Golden Images - Link Red Hat Gold Image to AWS Account to create AMI
Configure Terraform Enterprise
1.	Log into TFE
2.	cguser : https://tfe.cguser.capgroup.com/
3.	
cgftdev: 

4.	Reach out to the automation team and request for a new workspace if you are building out a new cluster.
5.	In your branch, create your tfvars file that has all of your variables for that cluster. Make sure the extension of the tf vars file is .auto.tvars 
6.	Example: ext-dev-east.inputs.auto.tfvars
7.	
TFE only picks up your variable file that has extension ".auto.tfvars" 
8.	Modify .tfvars to have 1 bastion, 0 masters, 0 appnodes, and enable_bootstrap_node to false. You only want to provision the Load Balancers so you can request DNS
Example : https://bitbucket.capgroup.com/projects/PLAT/repos/cg_tf_module_ocp4_aws/browse/int-qa-west.inputs.tfvars
9.	Edit these variables below: 
10.	
11.	
12.	cluster_name=ocp4-<ext or int>-<environment>-region
13.	environment=dev,qa, or prod
14.	
15.	num_masters=0
16.	num_apps=0
17.	num_gpus=0
18.	num_bastions=1
19.	enable_bootstrap_node=false
20.	
21.	
22.	#Update the vpc & region for the cluster you are deploying in
23.	vpc_id = "vpc-09a4d2d1cc9221ecc"
24.	region = "us-west-1"
25.	
26.	#Retrieve the Redhat Core OS AMIs. The AMI ID will be the same per region, so you can reference an existing tfvars file. 
27.	openshift_aws_ec2_master_ami              = "ami-0c1d2b5606111ac8c"
28.	openshift_aws_ec2_app_ami                 = "ami-0c1d2b5606111ac8c"
29.	openshift_aws_ec2_gpu_ami                 = "ami-0c1d2b5606111ac8c"
30.	openshift_aws_ec2_bootstrap_ami           = "ami-0c1d2b5606111ac8c"
31.	
32.	#Retrieve a RHEL7 AMI, you can use the same from an existing bastion server in the account. 
33.	openshift_aws_ec2_bastion_ami             = "ami-053b771067acaaac2"
34.	
35.	# In AWS console, go to EC2->Key Pairs, update with ssh key pair name 
36.	ssh_keypair                               = "ocp-int-prd-us-west-1"
37.	
38.	
39.	# Subnet ID in each AZ where master and App nodes will be provisioned
40.	# Two in west1: us-west-1a, us-west-1b, three in east1: us-east-1a, us-east-1b, us-east-1c
41.	ec2_subnets = [
42.	  "subnet-082d83de131751cca", #10.244.84.192/26
43.	  "subnet-07a9cdb6a08d0d07a", #10.244.85.0/26
44.	]
45.	
46.	# IPs to assign statically to master nodes. Must be listed in the same order as
47.	# the corresponding subnets in the ec2_subnets list. I.e., the first IP in this
48.	# list needs to belong to the first subnet in the ec2_subnets list and so on.
49.	# Also, make sure these IP's are not being used.
50.	ec2_master_ips = [
51.	  "10.244.84.252",
52.	  "10.244.85.61",
53.	  "10.244.84.251",
54.	]
55.	
56.	# Subnet ID in each AZ that the load balancers should be part of
57.	# Two in west1: us-west-1a, us-west-1b, three in east1: us-east-1a, us-east-1b, us-east-1c
58.	lb_subnets = [
59.	  "subnet-0ee59a3f121e8624a", 
60.	  "subnet-028e6ca41e2f414bb", 
61.	]
62.	
63.	
64.	# Tag variables
65.	
66.	customtags = {
67.	
68.	  "env-type"         = "prod"
69.	
70.	  # Cluster ID can be found in metadata.json after ignition files are
71.	  # generated by the OpenShift installer.
72.	  "kubernetes.io/cluster/ocp4-int-prod-west-9k2kw" = "owned"
}
73.	Connect the new workspace to your Bitbucket branch.
 
74.	Click on variables → Environment Variables → and set your aws access key. Make sure you checkbox the sensitive box
75.	Key: AWS_ACCESS_KEY_ID 			 Value:
Key: AWS_SECRET_ACCESS_KEY 		 Value: 
76.	Every time you commit code to your branch, terraform enterprise will automatically run a 'terraform plan', to apply the changes,  click on the confirm box in terraform enterprise. 
Create DNS entries for Load balancers and Masters
Get name of load balancer and create DNS names for this and the masters. Wait for DNS entries to be created
etcd-x records for each master (you'll know these because you assigned static addresses to them in the .tfvars file)
api and api-int CNAMES for the ELB
SRV record for etcd

 
Request wildcard certificate
1.	Generate a key & csr for wildcard certificate 
Example - request a wildcard certificate for "*.apps.ocp4-ext-qa-east.csp.capgroup.com" 
2.	 Create a servicenow ticket & attach csr generated to request for a certificate. 
Sample ServiceNow # - REQ0289495
Request Okta whitelist URI 
Sample Request below,  update the clientid and the oauth hostname for your cluster. 
SCTASK1075730
Env	ClientID
DEV	0oafv3v9e8QWurhHH0h7
QA	0oafv3r2yacBibUL90h7
PROD	0oa1eglpgs3Ieq0IS1d8
Configure Bastion Server
1.	Download the latest repos for Openshift 4.2
git clone https://bitbucket.capgroup.com/scm/plat/openshift.git
2.	Copy 4.2 directory from git into home directory
3.	cp -r openshift/ocp-provisioning/4.2/ ocp4-provisioning
4.	
cp ocp4-provisioning/bastion-prep-aws.sh .
5.	Update bastion-prep-aws.sh to download the latest openshift installer & openshift client.  Upate the openshift version, and the path of the 'wget' commands. Openshift moves their installer to a new folder when a new version is released. 
6.	#Link to view the latest version of Openshift 
7.	https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/
8.	
9.	
10.	#Update variable below in bastion-prep-aws.sh
11.	OPENSHIFT_VERSION=4.4.5
12.	wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-install-linux-$OPENSHIFT_VERSION.tar.gz
wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux-$OPENSHIFT_VERSION.tar.gz
13.	Run bastion-prep-aws.sh 
./bastion-prep-aws.sh
14.	Configure aws cli - enter access key & secret access key for the account/environment you are working in.
15.	aws configure
<Input access key & secret access key for the environment you are in >
16.	Install base agents & local unix accounts on bastion server. (You can skip this and do this step at the end) 
17.	#Install Base Agents & add local unix accounts
18.	https://confluence.capgroup.com/pages/viewpage.action?pageId=151293135
19.	
20.	#Create servicenow accounts for discovery
https://bitbucket.capgroup.com/projects/PLAT/repos/cloud_automation/browse/AWS/compute/ansible/playbooks/ocp311_post_playbook.yml
Create install-config.yaml
There needs to be a public hosted zone in Route53 with the indicated baseDomain (csp.capgroup.com). This needs to be accessible from the account in which the cluster is being provisioned. Route53 is not used in our deployments, however the installer will get angry and quit if it is not able to confirm that the zone exists.
In the account you are installing OCP4.2, if below route53 zone does not exist, create one.
 

1.	Create openshift install directory and generate install-config.yaml file
2.	mkdir /home/ec2-user/ocp4
3.	 
4.	./openshift-install create install-config --dir=/home/ec2-user/ocp4
5.	 
cp ocp4/install-config.yaml .
 
Enter ?
Logon to the url
Select AWS
Installer-Provided Infrastructure
Copy Pull Secret (https://cloud.redhat.com/openshift/install/pull-secret)

1.	Update values below in install-config.yaml 
1.	name - cluster name (ie. ocp4-int-dev-east1)
2.	machineCIDR - VPC CIDR RANGE 
3.	clusterNetwork CIDR - 192.168.0.0/17
4.	serviceNetwork - 192.168.240.0/20 
5.	region
6.	pullSecret
  
install-config.yaml  
apiVersion: v1
baseDomain: csp.capgroup.com
compute:
- hyperthreading: Enabled
  name: worker
  platform: {}
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  platform: {}
  replicas: 3
metadata:
  creationTimestamp: null
  name: ocp4-int-qa-west
networking:
  clusterNetwork:
  - cidr: 192.168.0.0/17
    hostPrefix: 23
  machineCIDR: 10.244.216.0/22
  networkType: OpenShiftSDN
  serviceNetwork:
  - 192.168.240.0/20
platform:
  aws:
    region: us-west-1
pullSecret: '{"auths":{"cloud.openshift.com":{"auth":"b3U0Tel==","email":"phlp@capgroup.com"}}}'

Create Manifests and Ignition Files 
TLS certificates are generated as part of the manifest and ignition file creation process. These certificates have a 24 hour expiration time. You must deploy the nodes within 24 hours of generating the manifests and ignition files.

1.	Modify generateocp4-manifest.sh (file could be on /home/ec2-user/ocp4/openshift/ocp-provisioning/4.2) for your appropriate directory structure, installer location and .tfvars files
2.	installation_dir=/home/ec2-user/ocp4
3.	openshift_installer=/home/ec2-user/openshift-install
tfvars_file=/home/ec2-user/cg_tf_module_ocp4_aws/int-dev-east.inputs.tfvars
4.	 Run generateocp4-manifest.sh 
https://bitbucket.capgroup.com/projects/PLAT/repos/openshift/browse/ocp-provisioning/4.2/generateocp4-manifest.sh
5.	./generateocp4-manifest.sh 
6.	
7.	
8.	# Summary of the script
9.	1. Delete 99_openshift-cluster-api_master-machines-*.yaml and 99_openshift-cluster-api_worker-machineset-*.yaml
10.	2. Mark masters as unschedulable
3. create ignition files
11.	Retrieve certificate from master.ign file and copy into .tfvars file. 
awk -F'"' -- '{print "ca_bundle = \"" $22 "\""}' master.ign 
VPC and Subnets
1.	All resources pertaining to the cluster need to be tagged with the infraID. This infra ID can be retrieved from metadata.json in the install folder. It will look something like dev-int-use1-aws-6c5vj. Tag will be
2.	Key: kubernetes.io/cluster/dev-int-use1-aws-6c5vj
Value: owned
3.	Update customtags section of tfvars to include the kubernetes tag. 
4.	customtags = {
5.	  "usage-id"         = "BB00000085"
6.	  "cost-center"      = 524068
7.	  "sec-profile"      = "normal"
8.	  "exp-date"         = "99-00-9999"
9.	  "sd-period"        = "na"
10.	  "ppmc-id"          = 69058
11.	  "env-type"         = "qa"
12.	  "cloud-dependency" = "cloudonly"
13.	  "site"             = "aws"
14.	  "toc"              = "ETOC"
15.	  # Cluster ID can be found in metadata.json after ignition files are
16.	  # generated by the OpenShift installer.
17.	  "kubernetes.io/cluster/ocp4-ext-qa-east-v2stz" = "owned"          <----------------------------------- Update this line
18.	}
19.	
20.	Since these resources already exist, they are not managed by Terraform. Manually apply the tags to the VPC and to the subnets used for load balancers. These will typically be /27s in our environments. us-east-1 will have 3. us-west-1 also has 3 but 2 of them are in the same AZ. We should only use 2 of them (one from each AZ).
21.	#Apply tag to aws resources below 
22.	- VPC
- InternalALB* Subnets

Host bootstrap ignition file on HTTP server
1.	Change directory into your installation directory and host python http server. 
2.	cd /home/ec2-user/ocp4
3.	
python -m SimpleHTTPServer 
Provision the Bootstrap and Master nodes with Terraform
1.	Modify .tfvars file to enable_bootstrap_node = true, num_masters = 3
2.	Modify .tvars file to include IP address of bastion host for httpserver = "http://<bastion_host_ip>:8000"
3.	Install terraform if it does not exist 
1.	https://bitbucket.capgroup.com/projects/PLAT/repos/cg_tf_module_ocp_aws/browse/cg_tf_module_ocp_aws/examples/ocp311-int-qa-east
4.	Run terraform apply, master and bootstrap nodes will be created and the cluster installation will start.
terraform apply -var-file=int-qa-west.inputs.tfvars
5.	Run the command below from bastion server to monitor the bootstrap & install process
./openshift-install wait-for bootstrap-complete --dir=ocp4 --log-level debug
6.	Here's an article for troubleshooting steps of the install fails.
https://access.redhat.com/articles/3787381
7.	Once the installation completes, remove the bootstrap node. Update .tfvars file, and set enable_bootstrap_node=false
8.	#Make sure variable "enable_bootstrap_node=false"
9.	
10.	#Running terraform apply will decommission the bootstrap server 
terraform apply -var-file=int-qa-west.inputs.tfvars 
Replace Default IngressController
The default ingresscontroller comes with scope set to External. This fails because we don't have any external subnets. Replace it with an ingresscontroller with scope set to Interna
1.	Patch DNS CRD to remove reference to Route53 if you are in an INTERNAL cluster 
oc patch dnses.config.openshift.io/cluster --type=merge --patch='{"spec": {"publicZone": null}}'
2.	Create ingresscontroller yaml file on bastion server
 
    apiVersion: operator.openshift.io/v1
    kind: IngressController
    metadata:
      namespace: openshift-ingress-operator
      name: default
    spec:
      endpointPublishingStrategy:
        type: LoadBalancerService
        loadBalancer:
          scope: Internal
3.	Delete default ingresscontroller -  The operator will recreate the ingresscontroller, so run this step and step #4 back to back. 

oc delete ingresscontroller default -n openshift-ingress-operator
4.	Create ingresscontroller - this will provision an Internal Load Balancer in AWS 
oc create -f ingress-operator.yml -n openshift-ingress-operator
5.	Open up a ticket to Network-NOC in ServiceNow, and request the wildcard dns entry to point to the CNAME of the ingress load balancer created (sample request: REQ0267195 ). 
6.	Change the subnet of the Load Balancer , and place them in the load balancer subnets. 
EC2 → Load Balancers → Instances → Edit Availability Zones 
Configuring Wildcard Certificate
1.	Create a ssl_direcotry and move the certificate and key into the directory
NOTE: For the certificate file, make sure you include the certificate followed by the intermediate certificate authority.
2.	$ pwd
3.	/home/ec2-user/ssl_cert
4.	
5.	$ ls
6.	ocp4-int-dev-west-wildcard-int-ca.crt  ocp4-int-dev-west-wildcard.key
7.	
#NOTE .crt file contains certificate + intermediate ca 
8.	 Digicert Intermediate Cert
digicert intermediate ca  
-----BEGIN CERTIFICATE-----
MIIEizCCA3OgAwIBAgIQDI7gyQ1qiRWIBAYe4kH5rzANBgkqhkiG9w0BAQsFADBh
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBH
MjAeFw0xMzA4MDExMjAwMDBaFw0yODA4MDExMjAwMDBaMEQxCzAJBgNVBAYTAlVT
MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxHjAcBgNVBAMTFURpZ2lDZXJ0IEdsb2Jh
bCBDQSBHMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANNIfL7zBYZd
W9UvhU5L4IatFaxhz1uvPmoKR/uadpFgC4przc/cV35gmAvkVNlW7SHMArZagV+X
au4CLyMnuG3UsOcGAngLH1ypmTb+u6wbBfpXzYEQQGfWMItYNdSWYb7QjHqXnxr5
IuYUL6nG6AEfq/gmD6yOTSwyOR2Bm40cZbIc22GoiS9g5+vCShjEbyrpEJIJ7RfR
ACvmfe8EiRROM6GyD5eHn7OgzS+8LOy4g2gxPR/VSpAQGQuBldYpdlH5NnbQtwl6
OErXb4y/E3w57bqukPyV93t4CTZedJMeJfD/1K2uaGvG/w/VNfFVbkhJ+Pi474j4
8V4Rd6rfArMCAwEAAaOCAVowggFWMBIGA1UdEwEB/wQIMAYBAf8CAQAwDgYDVR0P
AQH/BAQDAgGGMDQGCCsGAQUFBwEBBCgwJjAkBggrBgEFBQcwAYYYaHR0cDovL29j
c3AuZGlnaWNlcnQuY29tMHsGA1UdHwR0MHIwN6A1oDOGMWh0dHA6Ly9jcmw0LmRp
Z2ljZXJ0LmNvbS9EaWdpQ2VydEdsb2JhbFJvb3RHMi5jcmwwN6A1oDOGMWh0dHA6
Ly9jcmwzLmRpZ2ljZXJ0LmNvbS9EaWdpQ2VydEdsb2JhbFJvb3RHMi5jcmwwPQYD
VR0gBDYwNDAyBgRVHSAAMCowKAYIKwYBBQUHAgEWHGh0dHBzOi8vd3d3LmRpZ2lj
ZXJ0LmNvbS9DUFMwHQYDVR0OBBYEFCRuKy3QapJRUSVpAaqaR6aJ50AgMB8GA1Ud
IwQYMBaAFE4iVCAYlebjbuYP+vq5Eu0GF485MA0GCSqGSIb3DQEBCwUAA4IBAQAL
OYSR+ZfrqoGvhOlaOJL84mxZvzbIRacxAxHhBsCsMsdaVSnaT0AC9aHesO3ewPj2
dZ12uYf+QYB6z13jAMZbAuabeGLJ3LhimnftiQjXS8X9Q9ViIyfEBFltcT8jW+rZ
8uckJ2/0lYDblizkVIvP6hnZf1WZUXoOLRg9eFhSvGNoVwvdRLNXSmDmyHBwW4co
atc7TlJFGa8kBpJIERqLrqwYElesA8u49L3KJg6nwd3jM+/AVTANlVlOnAM2BvjA
jxSZnE0qnsHhfTuvcqdFuhOWKU4Z0BqYBvQ3lBetoxi6PrABDJXWKTUgNX31EGDk
92hiHuwZ4STyhxGs6QiA
-----END CERTIFICATE-----
9.	Create a secret that contains wildcard certificate and key
oc --namespace openshift-ingress create secret tls custom-certs-default --cert=ocp4-int-dev-west-wildcard-int-ca.crt --key=ocp4-int-dev-west-wildcard.key
10.	Update ingress controller to use the new secret created. 
oc patch ingresscontroller.operator default --type=merge -p '{"spec":{"defaultCertificate": {"name": "custom-certs-default"}}}' -n openshift-ingress-operator
11.	Create a "dummy" public hosted zone in AWS Route 53.
Domain Name: ocp4-ext-prod-east.csp.capgroup.com
12.	Retrieve the infra id, this will be used to tag Route53
13.	$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
ocp4-ext-qa-east-v2stz
14.	Tag the public hosted zone using aws cli. Replace the infraid with the value for your cluster. 
15.	aws route53 change-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH --add-tags Key=Name,Value=ocp4-ext-qa-east-v2stz-int
16.	
17.	aws route53 change-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH --add-tags Key=kubernetes.io/cluster/ocp4-ext-qa-east-v2stz,Value=owned
18.	
aws route53 list-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH
19.	Restart Ingress Operator dns crd in the cluster
oc delete pod ingress-operator-d8dc6fdf7-5wl9x -n openshift-ingress-operator
20.	Double check Route53, there should be 1 new  wildcard dns record added to your hosted zone.
External Load Balancer
If you cluster needs an External Load Balancer follow the instructions below
1.	Create a "dummy" public hosted zone in AWS Route 53.
Domain Name: ocp4-ext-prod-east.csp.capgroup.com
2.	Retrieve the infra id, this will be used to tag Route53
3.	$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
ocp4-ext-qa-east-v2stz
4.	Tag the public hosted zone using aws cli. Replace the infraid with the value for your cluster. 
5.	aws2 route53 change-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH --add-tags Key=Name,Value=ocp4-ext-qa-east-v2stz-int
6.	
7.	aws2 route53 change-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH --add-tags Key=kubernetes.io/cluster/ocp4-ext-qa-east-v2stz,Value=owned
8.	
aws2 route53 list-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH
9.	Add 2 Tags to Load Balancer External Subnets
10.	Key: kubernetes.io/cluster/ocp4-ext-qa-east-v2stz , Value: shared   
Key:  kubernetes.io/role/elb  , Value: 1
11.	Add 2 Tags to Load Balancer Internal subnets
12.	Key: kubernetes.io/cluster/ocp4-ext-qa-east-v2stz , Value: shared  
Key: kubernetes.io/role/internal-elb, Value: 1
13.	Edit the dns crd in the cluster
14.	oc edit dnses.config.openshift.io/cluster
15.	Add in the publicZone section with Route53 hosted zone ID. Example below. 
16.	apiVersion: config.openshift.io/v1
17.	kind: DNS
18.	metadata:
19.	  creationTimestamp: "2020-01-16T23:30:33Z"
20.	  generation: 3
21.	  name: cluster
22.	  resourceVersion: "25846016"
23.	  selfLink: /apis/config.openshift.io/v1/dnses/cluster
24.	  uid: 30cb8111-38b8-11ea-88a9-06366379e33f
25.	spec:
26.	  baseDomain: ocp4-ext-dev-west.csp.capgroup.com
27.	  privateZone:
28.	    tags:
29.	      Name: ocp4-ext-dev-west-p67zx-int
30.	      kubernetes.io/cluster/ocp4-ext-dev-west-p67zx: owned
31.	  publicZone:
32.	    id: Z1BCBTIB8QZ9CE
33.	status: {}
~           
34.	Create a public-ingress.yml file on the bastion server. Update the domain with your cluster name
35.	apiVersion: operator.openshift.io/v1
36.	kind: IngressController
37.	metadata:
38.	  namespace: openshift-ingress-operator
39.	  name: public-ingress
40.	spec:
41.	  domain: public.ocp4-ext-dev-west.csp.capgroup.com
42.	  endpointPublishingStrategy:
43.	    type: LoadBalancerService
44.	  routeSelector:
45.	    matchLabels:
      type: public
46.	Create the public-ingress.yml
oc create -f public-ingress.yml 
47.	Double check Route53, there should be 2 records in your hosted zone.
 
48.	Edit your router-public-ingress to add ip restrictions
oc edit svc/router-public-ingress -n openshift-ingress
49.	Here are the list of ips. Retrieve the loadBalancerSourceRanges: section
 
spec:
  clusterIP: 192.168.244.100
  externalTrafficPolicy: Local
  healthCheckNodePort: 30118
  loadBalancerSourceRanges:
  - 10.0.0.0/8
  - 23.4.240.0/24
  - 23.13.219.0/24
  - 23.43.60.0/24
  - 23.46.210.0/24
  - 23.48.168.0/22
  - 23.48.210.0/24
  - 23.50.48.0/20
  - 23.56.175.0/24
  - 23.77.230.0/24
  - 23.205.116.0/24
  - 23.219.39.0/24
  - 23.223.202.0/24
  - 60.254.143.0/24
  - 61.9.129.128/25
  - 63.151.118.0/24
  - 63.217.232.0/24
  - 63.233.60.0/23
  - 66.198.8.141/32
  - 66.198.8.142/31
  - 66.198.8.144/32
  - 67.220.142.19/32
  - 67.220.142.20/31
  - 67.220.142.22/32
  - 88.221.161.0/24
  - 95.100.169.0/24
  - 104.102.248.0/24
  - 104.112.3.0/24
  - 104.116.163.0/24
  - 104.119.189.0/24
  - 104.124.60.0/24
  - 117.104.138.0/24
  - 117.239.240.0/25
  - 172.232.0.0/24
  - 172.232.13.0/24
  - 184.26.137.0/24
  - 184.27.141.0/24
  - 184.28.156.0/24
  - 184.31.0.0/24
  - 184.84.242.21/32
  - 184.84.242.22/31
  - 184.84.242.32/30
  - 204.2.166.173/32
  - 204.2.166.174/31
  - 204.2.166.176/30
  - 204.2.166.180/32
  - 208.49.157.49/32
  - 208.49.157.50/31
  - 208.49.157.52/31
  - 208.49.157.54/32
  - 209.8.112.96/29
  - 209.8.112.104/31
  - 209.170.113.98/31
  - 209.170.113.100/31
  - 209.170.113.106/31
  - 209.170.113.108/32

Configuring Public  Certificate
1.	Create a ssl_directory and move the certificate and key into the directory
NOTE: For the certificate file, make sure you include the certificate followed by the intermediate certificate authority.
2.	$ pwd
3.	/home/ec2-user/ssl_cert
4.	
5.	$ ls
6.	cert.cer.txt                                    public_ocp4-ext-prod-east-csp-capgroup-com-int-ca.crt
public_ocp4-ext-prod-east-csp-capgroup-com.csr  public_ocp4-ext-prod-east-csp-capgroup-com.key
7.	 Check your Issuer DN. This Digicert Intermediate Cert is only for: 
Country = US, Organization = DigiCert Inc, Common Name = DigiCert Global CA G2
8.	digicert intermediate ca  
9.	-----BEGIN CERTIFICATE-----
10.	MIIEizCCA3OgAwIBAgIQDI7gyQ1qiRWIBAYe4kH5rzANBgkqhkiG9w0BAQsFADBh
11.	MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
12.	d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBH
13.	MjAeFw0xMzA4MDExMjAwMDBaFw0yODA4MDExMjAwMDBaMEQxCzAJBgNVBAYTAlVT
14.	MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxHjAcBgNVBAMTFURpZ2lDZXJ0IEdsb2Jh
15.	bCBDQSBHMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANNIfL7zBYZd
16.	W9UvhU5L4IatFaxhz1uvPmoKR/uadpFgC4przc/cV35gmAvkVNlW7SHMArZagV+X
17.	au4CLyMnuG3UsOcGAngLH1ypmTb+u6wbBfpXzYEQQGfWMItYNdSWYb7QjHqXnxr5
18.	IuYUL6nG6AEfq/gmD6yOTSwyOR2Bm40cZbIc22GoiS9g5+vCShjEbyrpEJIJ7RfR
19.	ACvmfe8EiRROM6GyD5eHn7OgzS+8LOy4g2gxPR/VSpAQGQuBldYpdlH5NnbQtwl6
20.	OErXb4y/E3w57bqukPyV93t4CTZedJMeJfD/1K2uaGvG/w/VNfFVbkhJ+Pi474j4
21.	8V4Rd6rfArMCAwEAAaOCAVowggFWMBIGA1UdEwEB/wQIMAYBAf8CAQAwDgYDVR0P
22.	AQH/BAQDAgGGMDQGCCsGAQUFBwEBBCgwJjAkBggrBgEFBQcwAYYYaHR0cDovL29j
23.	c3AuZGlnaWNlcnQuY29tMHsGA1UdHwR0MHIwN6A1oDOGMWh0dHA6Ly9jcmw0LmRp
24.	Z2ljZXJ0LmNvbS9EaWdpQ2VydEdsb2JhbFJvb3RHMi5jcmwwN6A1oDOGMWh0dHA6
25.	Ly9jcmwzLmRpZ2ljZXJ0LmNvbS9EaWdpQ2VydEdsb2JhbFJvb3RHMi5jcmwwPQYD
26.	VR0gBDYwNDAyBgRVHSAAMCowKAYIKwYBBQUHAgEWHGh0dHBzOi8vd3d3LmRpZ2lj
27.	ZXJ0LmNvbS9DUFMwHQYDVR0OBBYEFCRuKy3QapJRUSVpAaqaR6aJ50AgMB8GA1Ud
28.	IwQYMBaAFE4iVCAYlebjbuYP+vq5Eu0GF485MA0GCSqGSIb3DQEBCwUAA4IBAQAL
29.	OYSR+ZfrqoGvhOlaOJL84mxZvzbIRacxAxHhBsCsMsdaVSnaT0AC9aHesO3ewPj2
30.	dZ12uYf+QYB6z13jAMZbAuabeGLJ3LhimnftiQjXS8X9Q9ViIyfEBFltcT8jW+rZ
31.	8uckJ2/0lYDblizkVIvP6hnZf1WZUXoOLRg9eFhSvGNoVwvdRLNXSmDmyHBwW4co
32.	atc7TlJFGa8kBpJIERqLrqwYElesA8u49L3KJg6nwd3jM+/AVTANlVlOnAM2BvjA
33.	jxSZnE0qnsHhfTuvcqdFuhOWKU4Z0BqYBvQ3lBetoxi6PrABDJXWKTUgNX31EGDk
34.	92hiHuwZ4STyhxGs6QiA
35.	-----END CERTIFICATE-----
36.	Create a kubernetes secret that contains public certificate and key
oc --namespace openshift-ingress create secret tls public-certs-default --cert=public_ocp4-ext-prod-east-csp-capgroup-com-int-ca.crt --key=public_ocp4-ext-prod-east-csp-capgroup-com.key
37.	Update the public-ingress  controller to use the new certificate secret created from previous step. 
oc patch ingresscontroller.operator public-ingress --type=merge -p '{"spec":{"defaultCertificate": {"name": "public-certs-default"}}}' -n openshift-ingress-operator
Create machine set for worker nodes
OpenShift creates an IAM user automatically to do things like create machines in the machine set. This user needs to be given access to the EBS volume KMS key because the nodes that it spins up will have encrypted EBS volumes. Find the user in IAM users and add it to the key policy for the EBS volume CMK. The IAM user's name will be something like ocp4-int-qa-east-rl4-openshift-machine-api-aws-mvvzv. 
Go to IAM and select the user, then copy the ARN.
Go to KMS and select the EBS volumn key (ie. ocp-int-dev/useast1/volume/key), then click Edit
Added the user (the copied ARN) to "Allow user of the key" and "Allow attachment of persistent resources".

For each AZ, create a machineset (worker-machineset-1a.yaml) like the below example. Change the dev-int-use1-aws-6c5vj to whatever the clusters ID is. This can be found in metadata.json which is created along with the ignition files. You'll also need to fix the following values:
•	instanceType
•	availabilityZone
•	region
•	worker security group id (example: worker-sg-ocp4-int-dev-east1)
•	subnet
•	IAM instance policy (this is the IAM role assigned to the workers. example: ocp4-int-dev-east1-WorkerIamRole)
•	master0 ami

machineset-int-qa-west-us-west-1a  
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: ocp4-int-qa-west-hqj5n
  name: ocp4-int-qa-west-hqj5n-worker-us-west-1a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: ocp4-int-qa-west-hqj5n
      machine.openshift.io/cluster-api-machineset: ocp4-int-qa-west-hqj5n-worker-us-west-1a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: ocp4-int-qa-west-hqj5n
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: ocp4-int-qa-west-hqj5n-worker-us-west-1a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/worker: "" 
      providerSpec:
        value:
          ami:
            id: ami-0c1d2b5606111ac8c
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: ocp4-int-qa-west-worker-instance-profile
          instanceType: m5.xlarge
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: us-west-1a
            region: us-west-1
          securityGroups:
          - id: sg-0d896016c47434f38
          subnet:
            filters:
              - name: tag:Name
                values:
                  - subnet4-OCPCluster-us-west-1a-qa
          tags:
            - name: kubernetes.io/cluster/ocp4-int-qa-west-hqj5n
              value: owned
          userDataSecret:
            name: worker-user-data

1.	Create the machine set 
oc create -f worker-machineset-1a.yaml -n openshift-machine-api
2.	Create machinesets for each AZ
3.	Update Router to run x instances of the ingress pods
oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{"spec":{"replicas": 3}}' --type=merge
Configure Cluster Autoscaler
1.	Create a cluster autoscaler , yaml file is below.
clusterautoscaler yaml 
apiVersion: autoscaling.openshift.io/v1
kind: ClusterAutoscaler
metadata:
  name: default
spec:
  balanceSimilarNodeGroups: true
  resourceLimits:
    maxNodesTotal: 20
  scaleDown:
    delayAfterAdd: 10m
    delayAfterDelete: 5m
    delayAfterFailure: 30s
    enabled: true
    unneededTime: 10m
2.	Create the cluster autoscaler from yaml file
oc create -f cluster-autoscaler.yaml
3.	Create a machineautoscaler yaml for each machineset. So if you have 3 machinesets, you will need to create 3 machine autoscaler to point to each machineset. 
machine auto scaler yaml 
apiVersion: autoscaling.openshift.io/v1beta1
kind: MachineAutoscaler
metadata:
  name: autoscale-m5-xlarge-us-west-1b
  namespace: openshift-machine-api
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet
    name: ocp4-int-qa-west-hqj5n-worker-us-west-1b
Creating default network policies for a new project
This policy will make the namespace / project allow only connection from the OCP ingress controller.
Create default network policy project template
oc adm create-bootstrap-project-template -o yaml > template.yaml
oc create -f template.yaml -n openshift-config
oc edit project.config.openshift.io/cluster

Update the spec section to include the projectRequestTemplate and name parameters, and set the name of your uploaded project template. The default name is project-request.
 
apiVersion: config.openshift.io/v1
kind: Project
metadata:
  ...
spec:
  projectRequestTemplate:
    name: project-request

Update objects section with network policies. In the following example, the objects parameter collection includes NetworkPolicy to restrict traffic from pods in other projects.
oc edit template project-request -n openshift-config

oc process project-request | oc create -f -
 
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-same-namespace
  spec:
    ingress:
    - from:
      - podSelector: {}
    podSelector: null
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-openshift-ingress
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
    podSelector: {}
    policyTypes:
    - Ingress
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-openshift-monitoring
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: monitoring
    podSelector: {}
    policyTypes:
    - Ingress
Do this step ONLY if you need to enable self-service for development cluster with resource quotas and limiting ranges.
project-request.yaml
apiVersion: v1
kind: Template
metadata:
  creationTimestamp: null
  name: project-request
objects:
- apiVersion: project.openshift.io/v1
  kind: Project
  metadata:
    annotations:
      openshift.io/node-selector: node-role.kubernetes.io/app=
      openshift.io/description: ${PROJECT_DESCRIPTION}
      openshift.io/display-name: ${PROJECT_DISPLAYNAME}
      openshift.io/requester: ${PROJECT_REQUESTING_USER}
      cg/project-type: ephemeral
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: cg-default-quota
  spec:
    hard:
      limits.cpu: '2'
      limits.memory: 4Gi
- apiVersion: "v1"
  kind: "LimitRange"
  metadata:
    name: "default-limitrange-for-builders"
  spec:
    limits:
      - type: "Container"
        #these defaults are needed to avoid S2I builders blowing up with "Error creating: pods <buildpod> is forbidden: failed quota: <quotaname>: must specify limits.cpu,limits.memory"
        default:
          cpu: "300m"
          memory: "1200Mi"
        defaultRequest:
          cpu: "100m"
          memory: "250Mi"
- apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: system:image-pullers
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:image-puller
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:serviceaccounts:${PROJECT_NAME}
- apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: system:image-builders
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:image-builder
  subjects:
  - kind: ServiceAccount
    name: builder
    namespace: ${PROJECT_NAME}
- apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: system:deployers
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:deployer
  subjects:
  - kind: ServiceAccount
    name: deployer
    namespace: ${PROJECT_NAME}
- apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: admin
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: ${PROJECT_ADMIN_USER}
parameters:
- name: PROJECT_NAME
- name: PROJECT_DISPLAYNAME
- name: PROJECT_DESCRIPTION
- name: PROJECT_ADMIN_USER
- name: PROJECT_REQUESTING_USER

Whitelist/ Blacklist Docker Registries
oc edit image.config.openshift.io/cluster
Update spec section to include below and save the contents
OCP4 DEV 
spec:
  registrySources:
    allowedRegistries:
    - cgtstregistry.capgroup.com
    - cgdevregistry.capgroup.com
    - cgprdregistry.capgroup.com
    - cgregistry.capgroup.com
    - registry.access.redhat.com
    - registry.redhat.io
    - registry.connect.redhat.com
    - quay.io
    - image-registry.openshift-image-registry.svc:5000
    - registry-auth.twistlock.com
    - docker.io
    - k8s.gcr.io
    - gcr.io
    - nvcr.io
    - 410997643304.dkr.ecr.us-east-1.amazonaws.com
    - 410997643304.dkr.ecr.us-east-2.amazonaws.com
    - 410997643304.dkr.ecr.us-west-1.amazonaws.com
    - 410997643304.dkr.ecr.us-west-2.amazonaws.com


OCP4 QA 
spec:
  registrySources:
    allowedRegistries:
    - cgtstregistry.capgroup.com
    - cgprdregistry.capgroup.com
    - cgregistry.capgroup.com
    - registry.access.redhat.com
    - registry.redhat.io
    - registry.connect.redhat.com
    - quay.io
    - image-registry.openshift-image-registry.svc:5000
    - registry-auth.twistlock.com
    - docker.io
    - k8s.gcr.io
    - gcr.io
    - nvcr.io
    - 410997643304.dkr.ecr.us-east-1.amazonaws.com
    - 410997643304.dkr.ecr.us-east-2.amazonaws.com
    - 410997643304.dkr.ecr.us-west-1.amazonaws.com
    - 410997643304.dkr.ecr.us-west-2.amazonaws.com

OCP4 Prod 
spec:
  registrySources:
    allowedRegistries:
    - cgprdregistry.capgroup.com
    - cgregistry.capgroup.com
    - registry.access.redhat.com
    - registry.redhat.io
    - registry.connect.redhat.com
    - quay.io
    - image-registry.openshift-image-registry.svc:5000
    - registry-auth.twistlock.com
    - docker.io
    - k8s.gcr.io
    - gcr.io
    - nvcr.io
    - 410997643304.dkr.ecr.us-east-1.amazonaws.com
    - 410997643304.dkr.ecr.us-east-2.amazonaws.com
    - 410997643304.dkr.ecr.us-west-1.amazonaws.com
    - 410997643304.dkr.ecr.us-west-2.amazonaws.com
Create Storage Class
1.	On the bastion server, create a file with the contents below. Make sure you update the variables listed depending on the environment you are in. 
1.	kmsKeyId - arn of kms key id for volume encryption
2.	Retrieve the masteriam role and add it to the kms key policy: arn:aws:iam::775473764191:role/ocp4-ext-dev-west-MasterIamRole 
aws-ebs-sc.yml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
    storageclass.kubernetes.io/is-default-class: "true"
  name: aws-ebs-encrypted
parameters:
  encrypted: "true"
  kmsKeyId: arn:aws:kms:us-east-1:836816519470:key/540bbca4-083f-436b-aa79-99d8213294e2
  type: gp2
provisioner: kubernetes.io/aws-ebs
reclaimPolicy: Delete
volumeBindingMode: Immediate
2.	Create the storage class from step #1
oc create -f aws-ebs-sc.yml
3.	Set the  'gp2' storage class to not default
 oc patch storageclass gp2 -p '{"metadata":{"annotations":{"storageclass.beta.kubernetes.io/is-default-class":"false","storageclass.kubernetes.io/is-default-class":"false"}}}'
3rd party integrations
Okta
https://docs.openshift.com/container-platform/4.1/authentication/identity_providers/configuring-oidc-identity-provider.html
1.	Run ansible playbook to configure ad sync. Go to cyberark and retrieve the password to run the playbook
# ansible-playbook  4.2/ocp_ops/configure-adsync.yaml -e env=dev --ask-vault-pass
Variable	Values
env	'dev' or 'qa' or 'prod'
--ask-vault-pass	check cyberark

 

Splunk
Installation Steps:
1  From the bastion host, download the installation source from Bitbucket to '/home/<SA>'.
git clone https://bitbucket.capgroup.com/scm/plat/openshift.gi

2 From the bastion host, cd to /openshift/ocp-provisioning/4.2 directory and run the following command passing in the cluster_name.
ansible-playbook install-splunk-connect-k8s.yaml -e cluster_name=<Name-of-cluster>
Dynatrace
to start of metadata 
Installation Steps:
1  From the bastion host, download the installation source from Bitbucket to '/home/<SA>'.
git clone https://bitbucket.capgroup.com/scm/plat/openshift.git

2 From the bastion host, cd to /openshift/ocp-provisioning/4.2 directory and run the following command passing in the cluster_name.
ansible-playbook install-dynatrace-agent.yml -e cluster_name=<Name-of-cluster>
3  Create cluster management zone
https://confluence.capgroup.com/x/elAnD

4  Enable kubernetes resources monitoring
https://confluence.capgroup.com/x/BJzuCw


AlertManager (Email Notification)
This is to enable email notification for all critical alerts from Prometheus alert monitoring
1  Create file alertmanager.yaml with the following content.
alertmanager.yaml  
global:
  resolve_timeout: 5m
  http_config: {}
  smtp_hello: localhost
  smtp_require_tls: true
  smtp_from: "CPZINFBB00000085S004@capgroup.com"
  smtp_smarthost: "smtprelay1-1.cguser.capgroup.com:25"
  smtp_auth_username: "CPZINFBB00000085S004@capgroup.com"
  smtp_auth_password: "<password-from-cyberark>"
route:
  group_wait: 30s
  group_interval: 5m
  group_by: ['alertname', 'cluster', 'service']
  repeat_interval: 4h
  receiver: default
  routes:
  - match:
      alertname: Watchdog
    repeat_interval: 5m
    receiver: watchdog
  - match:
      container: csi-attacher
    receiver: default
  - match:
      severity: critical
    receiver: pds-team
receivers:
- name: default
- name: watchdog
- name: pds-team
  email_configs:
  - to: "<Get-EMAIL-DL>"

# Mute any warning-level notifications if the same alert is
# already critical.
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'cluster', 'service']
2  Get the password for account "CPZINFBB00000085S004@capgroup.com" from CyberArk and update smtp_auth_password: "<password-from-cyberark>"  with the password
3  Update the email distribution list to: "<Get-EMAIL-DL>" depending on the environment:
PROD: PDS_Alert_Notification_Prod@capgroup.com 
NON-PROD: PDS_Alert_Notification_Non-Prod@capgroup.com 
4  Execute the following command to apply the configuration.
oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run -o=yaml | oc -n openshift-monitoring replace secret --filename=-
Image Pruning
Image Pruning
This cronjob will prune images keeping the last 3 revisions.  The cronjob will run nightly starting at midnight.  This is based on ubi image with OC client installed.  The image is cgprdregistry.capgroup.com/ubi7/ubi-occlient.
1  Log into Bastion host
2  Authenticate using your 2A token
3  Create configuration file named 'cronjob-prune-images.yml' with the contents below
cronjob-prune-images.yml  
---
# This cronjob will prune images keeping the last 3 revisions.  The job will run every night at midnight
#
# Create project and apply the configuration
# oc new-project cluster-ops
# oc process -f cronjob-prune-images.yml -p NAMESPACE="cluster-ops" -p JOB_SERVICE_ACCOUNT=pruner | oc apply -f-
# To delete the project
# oc process -f cronjob-prune-images.yml -p NAMESPACE="cluster-ops" -p JOB_SERVICE_ACCOUNT=pruner | oc delete -f-
#
# Manually create job (oc get cronjob to list)
# oc create job --from=cronjob.batch/cronjob-prune-images <job-name>
# Manully delete job (oc get all to list all existent ones)
# oc delete job.batch/bkd-pruner-job
kind: Template
apiVersion: v1
metadata:
  name: cronjob-prune-images
  annotations:
    description: Scheduled Task to Prune Images from Internal Docker Registry
    iconClass: icon-shadowman
    tags: management,cronjob,prune,images
objects:
- kind: CronJob
  apiVersion: batch/v1beta1
  metadata:
    name: "${JOB_NAME}"
  spec:
    schedule: "${SCHEDULE}"
    concurrencyPolicy: Forbid
    successfulJobsHistoryLimit: "${{SUCCESS_JOBS_HISTORY_LIMIT}}"
    failedJobsHistoryLimit: "${{FAILED_JOBS_HISTORY_LIMIT}}"
    jobTemplate:
      spec:
        template:
          spec:
            containers:
            - name: "${JOB_NAME}"
              image: "${IMAGE}:${IMAGE_TAG}"
              command:
              - "/bin/bash"
              - "-c"
              - oc adm prune images --keep-tag-revisions=$IMAGE_PRUNE_KEEP_TAG_REVISIONS
                --keep-younger-than=$IMAGE_PRUNE_KEEP_YOUNGER_THAN --confirm --force-insecure
              env:
              - name: IMAGE_PRUNE_KEEP_TAG_REVISIONS
                value: "${IMAGE_PRUNE_KEEP_TAG_REVISIONS}"
              - name: IMAGE_PRUNE_KEEP_YOUNGER_THAN
                value: "${IMAGE_PRUNE_KEEP_YOUNGER_THAN}"
            restartPolicy: Never
            terminationGracePeriodSeconds: 30
            activeDeadlineSeconds: 500
            dnsPolicy: ClusterFirst
            serviceAccountName: "${JOB_SERVICE_ACCOUNT}"
            serviceAccount: "${JOB_SERVICE_ACCOUNT}"
- kind: ClusterRoleBinding
  apiVersion: v1
  metadata:
    name: system:image-pruners
    labels:
      template: "cronjob-prune-images"
  roleRef:
    name: cluster-admin
  subjects:
  - kind: ServiceAccount
    name: ${JOB_SERVICE_ACCOUNT}
  userNames:
  - system:serviceaccount:${NAMESPACE}:${JOB_SERVICE_ACCOUNT}
- kind: ServiceAccount
  apiVersion: v1
  metadata:
    name: ${JOB_SERVICE_ACCOUNT}
    labels:
      template: "cronjob-prune-images"
parameters:
- name: JOB_NAME
  displayName: Job Name
  description: Name of the Scheduled Job to Create.
  value: cronjob-prune-images
  required: true
- name: SCHEDULE
  displayName: Cron Schedule
  description: Cron Schedule to Execute the Job
  value: 0 0 * * *
  required: true
- name: JOB_SERVICE_ACCOUNT
  displayName: Service Account Name
  description: Name of the Service Account To Exeucte the Job As.
  value: pruner
  required: true
- name: IMAGE_PRUNE_KEEP_TAG_REVISIONS
  displayName: Number of Tag Revisions
  description: Specify the number of image revisions for a tag in an image stream
    that will be preserved.
  value: '3'
  required: true
- name: IMAGE_PRUNE_KEEP_YOUNGER_THAN
  displayName: Minimum Age of an Image
  description: The minimum age of an image for it to be considered a candidate for
    pruning
  value: 1h0m0s
  required: true
- name: SUCCESS_JOBS_HISTORY_LIMIT
  displayName: Successful Job History Limit
  description: The number of successful jobs that will be retained
  value: '5'
  required: true
- name: FAILED_JOBS_HISTORY_LIMIT
  displayName: Failed Job History Limit
  description: The number of failed jobs that will be retained
  value: '5'
  required: true
- name: "NAMESPACE"
  displayName: "Namespace where this is deployed"
  description: "Namespace where this is deployed."
  value: "cluster-ops"
  required: true
- name: "IMAGE"
  displayName: "Image"
  description: "Image to use for the container."
  required: true
  value: "cgprdregistry.capgroup.com/ubi7/ubi-occlient"
- name: "IMAGE_TAG"
  displayName: "Image Tag"
  description: "Image Tag to use for the container."
  required: true
  value: "latest"

labels:
  template: cronjob-prune-images
4  Create project and apply the configuration
oc new-project cluster-ops
5  Install the cronjob configuration
oc process -f cronjob-prune-images.yml -p NAMESPACE="cluster-ops" -p JOB_SERVICE_ACCOUNT=pruner | oc apply -f-

External Load Balancer
If you cluster needs an External Load Balancer follow the instructions below
1.	Create a "dummy" public hosted zone in AWS Route 53.
Domain Name: ocp4-ext-prod-east.csp.capgroup.com
2.	Retrieve the infra id, this will be used to tag Route53
3.	$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
ocp4-ext-qa-east-v2stz
4.	Tag the public hosted zone using aws cli. Replace the infraid with the value for your cluster. 
5.	aws2 route53 change-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH --add-tags Key=Name,Value=ocp4-ext-qa-east-v2stz-int
6.	
7.	aws2 route53 change-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH --add-tags Key=kubernetes.io/cluster/ocp4-ext-qa-east-v2stz,Value=owned
8.	
aws2 route53 list-tags-for-resource --resource-type hostedzone --resource-id Z06208992BDB37F184WYH
9.	Add 2 Tags to Load Balancer External Subnets
10.	Key: kubernetes.io/cluster/ocp4-ext-qa-east-v2stz , Value: shared   
Key:  kubernetes.io/role/elb  , Value: 1
11.	Add 2 Tags to Load Balancer Internal subnets
12.	Key: kubernetes.io/cluster/ocp4-ext-qa-east-v2stz , Value: shared  
Key: kubernetes.io/role/internal-elb, Value: 1
13.	Edit the dns crd in the cluster
14.	oc edit dnses.config.openshift.io/cluster
15.	Add in the publicZone section with Route53 hosted zone ID. Example below. 
16.	apiVersion: config.openshift.io/v1
17.	kind: DNS
18.	metadata:
19.	  creationTimestamp: "2020-01-16T23:30:33Z"
20.	  generation: 3
21.	  name: cluster
22.	  resourceVersion: "25846016"
23.	  selfLink: /apis/config.openshift.io/v1/dnses/cluster
24.	  uid: 30cb8111-38b8-11ea-88a9-06366379e33f
25.	spec:
26.	  baseDomain: ocp4-ext-dev-west.csp.capgroup.com
27.	  privateZone:
28.	    tags:
29.	      Name: ocp4-ext-dev-west-p67zx-int
30.	      kubernetes.io/cluster/ocp4-ext-dev-west-p67zx: owned
31.	  publicZone:
32.	    id: Z1BCBTIB8QZ9CE
33.	status: {}
~           
34.	Create a public-ingress.yml file on the bastion server. Update the domain with your cluster name
35.	apiVersion: operator.openshift.io/v1
36.	kind: IngressController
37.	metadata:
38.	  namespace: openshift-ingress-operator
39.	  name: public-ingress
40.	spec:
41.	  domain: public.ocp4-ext-dev-west.csp.capgroup.com
42.	  endpointPublishingStrategy:
43.	    type: LoadBalancerService
44.	  routeSelector:
45.	    matchLabels:
      type: public
46.	Create the public-ingress.yml
oc create -f public-ingress.yml 
47.	Double check Route53, there should be 2 records in your hosted zone.
 

Configuring Public  Certificate
1.	Create a ssl_directory and move the certificate and key into the directory
NOTE: For the certificate file, make sure you include the certificate followed by the intermediate certificate authority.
2.	$ pwd
3.	/home/ec2-user/ssl_cert
4.	
5.	$ ls
6.	cert.cer.txt                                    public_ocp4-ext-prod-east-csp-capgroup-com-int-ca.crt
public_ocp4-ext-prod-east-csp-capgroup-com.csr  public_ocp4-ext-prod-east-csp-capgroup-com.key
7.	 Check your Issuer DN. This Digicert Intermediate Cert is only for: 
Country = US, Organization = DigiCert Inc, Common Name = DigiCert Global CA G2
8.	digicert intermediate ca  
9.	-----BEGIN CERTIFICATE-----
10.	MIIEizCCA3OgAwIBAgIQDI7gyQ1qiRWIBAYe4kH5rzANBgkqhkiG9w0BAQsFADBh
11.	MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
12.	d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBH
13.	MjAeFw0xMzA4MDExMjAwMDBaFw0yODA4MDExMjAwMDBaMEQxCzAJBgNVBAYTAlVT
14.	MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxHjAcBgNVBAMTFURpZ2lDZXJ0IEdsb2Jh
15.	bCBDQSBHMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANNIfL7zBYZd
16.	W9UvhU5L4IatFaxhz1uvPmoKR/uadpFgC4przc/cV35gmAvkVNlW7SHMArZagV+X
17.	au4CLyMnuG3UsOcGAngLH1ypmTb+u6wbBfpXzYEQQGfWMItYNdSWYb7QjHqXnxr5
18.	IuYUL6nG6AEfq/gmD6yOTSwyOR2Bm40cZbIc22GoiS9g5+vCShjEbyrpEJIJ7RfR
19.	ACvmfe8EiRROM6GyD5eHn7OgzS+8LOy4g2gxPR/VSpAQGQuBldYpdlH5NnbQtwl6
20.	OErXb4y/E3w57bqukPyV93t4CTZedJMeJfD/1K2uaGvG/w/VNfFVbkhJ+Pi474j4
21.	8V4Rd6rfArMCAwEAAaOCAVowggFWMBIGA1UdEwEB/wQIMAYBAf8CAQAwDgYDVR0P
22.	AQH/BAQDAgGGMDQGCCsGAQUFBwEBBCgwJjAkBggrBgEFBQcwAYYYaHR0cDovL29j
23.	c3AuZGlnaWNlcnQuY29tMHsGA1UdHwR0MHIwN6A1oDOGMWh0dHA6Ly9jcmw0LmRp
24.	Z2ljZXJ0LmNvbS9EaWdpQ2VydEdsb2JhbFJvb3RHMi5jcmwwN6A1oDOGMWh0dHA6
25.	Ly9jcmwzLmRpZ2ljZXJ0LmNvbS9EaWdpQ2VydEdsb2JhbFJvb3RHMi5jcmwwPQYD
26.	VR0gBDYwNDAyBgRVHSAAMCowKAYIKwYBBQUHAgEWHGh0dHBzOi8vd3d3LmRpZ2lj
27.	ZXJ0LmNvbS9DUFMwHQYDVR0OBBYEFCRuKy3QapJRUSVpAaqaR6aJ50AgMB8GA1Ud
28.	IwQYMBaAFE4iVCAYlebjbuYP+vq5Eu0GF485MA0GCSqGSIb3DQEBCwUAA4IBAQAL
29.	OYSR+ZfrqoGvhOlaOJL84mxZvzbIRacxAxHhBsCsMsdaVSnaT0AC9aHesO3ewPj2
30.	dZ12uYf+QYB6z13jAMZbAuabeGLJ3LhimnftiQjXS8X9Q9ViIyfEBFltcT8jW+rZ
31.	8uckJ2/0lYDblizkVIvP6hnZf1WZUXoOLRg9eFhSvGNoVwvdRLNXSmDmyHBwW4co
32.	atc7TlJFGa8kBpJIERqLrqwYElesA8u49L3KJg6nwd3jM+/AVTANlVlOnAM2BvjA
33.	jxSZnE0qnsHhfTuvcqdFuhOWKU4Z0BqYBvQ3lBetoxi6PrABDJXWKTUgNX31EGDk
34.	92hiHuwZ4STyhxGs6QiA
35.	-----END CERTIFICATE-----
36.	Create a kubernetes secret that contains public certificate and key
oc --namespace openshift-ingress create secret tls public-certs-default --cert=public_ocp4-ext-prod-east-csp-capgroup-com-int-ca.crt --key=public_ocp4-ext-prod-east-csp-capgroup-com.key
37.	Update the public-ingress  controller to use the new certificate secret created from previous step. 
oc patch ingresscontroller.operator public-ingress --type=merge -p '{"spec":{"defaultCertificate": {"name": "public-certs-default"}}}' -n openshift-ingress-operator
Setting up CGBUILD namespace
https://docs.openshift.com/container-platform/4.1/builds/creating-build-inputs.html#builds-docker-credentials-private-registries_creating-build-inputs
1.	Create cgbuild namespace
oc new-project cgbuild
2.	Create cgbuild service account and grant edit access
3.	$ oc create sa cgbuild
4.	
$ oc policy add-role-to-user edit system:serviceaccount:cgbuild:cgbuild
5.	Get secrets required to connect to bitbucket, nexus registry & twistlock. Retrieve this from an existing cluster that has cgbuild configured
6.	bitbucket-read             kubernetes.io/basic-auth              2      83d
7.	cgregistry                 kubernetes.io/dockerconfigjson        1      83d
twistlock-scan2            kubernetes.io/basic-auth              2      34d
8.	Create those secrets from previous step in the new cgbuild namespace.
9.	In cgbuild namespace, link the nexus secret to builder service account
10.	$ oc secrets link builder cgregistry
Portworx
Portworx 3.2.1 Installation steps - OCP 4.2
Twistlock
Conjur
Currently Conjur and Openshift 4 integration is not fully supported.  There are currently two patterns for using Conjur with Openshift.  The difference between the two method is how the application/namespace authenticates with Conjur.
Method 1: Authentication through API key
This method involves using an API identity to authenticate with Conjur.  Secrets are pulled from the Conjur API through Summon or Conjur libraries.
API identities and configuration are set via configuration file .conjurrc or as environmental variables:
•	CONJUR_APPLIANCE_URL
•	CONJUR_AUTHN_URL
•	CONJUR_CERT_FILE
•	CONJUR_ACCOUNT
•	CONJUR_AUTHN_LOGIN
•	CONJUR_AUTHN_API_KEY
apiVersion: v1
kind: BuildConfig
metadata:
  name: secret-example-bc
spec:
  strategy:
    sourceStrategy:
      env:
      - name: CONJUR_APPLIANCE_URL
        value: https://conjur.cgftdev.com/
      - name: CONJUR_AUTHN_LOGIN
        value: FOO@user
      - name: CONJUR_AUTHN_API_KEY
        valueFrom:
          secretKeyRef:
            key: myval
            name: mysecret
Summon or the Conjur libraries will automatically reference these environmental variables or .conjurrc file to authenticate with Conjur.
Dockerfile - Install Summon 
#---install summon and summon-conjur---#
RUN curl -sSL https://raw.githubusercontent.com/cyberark/summon/master/install.sh \
      | env TMPDIR=$(mktemp -d) bash && \
    curl -sSL https://raw.githubusercontent.com/cyberark/summon-conjur/master/install.sh \
      | env TMPDIR=$(mktemp -d) bash
ENV PATH="/usr/local/lib/summon:${PATH}"
Method 2: Authentication through Kubernetes API
This method involves using the Kubernetes API along with the Conjur API to inject an access token into a Conjur sidecar running inside the namespace.  To achieve this, a Conjur namespace is required for each Openshift Cluster.  The Conjur namespace will have at least one Conjur follower which will replicate the secret store from the Conjur master that exist outside of the OCP cluster.
The application namespace will pull secrets from Conjur using Summon via a persistent sidecar or one that exist only during application initialization.  The sidecar authenticates to Conjur happens via the Kubernetes API.  When authenticating, a secret file will be loaded on the sidecar mounted storage.  The mounted storage is also mounted on the container running the app, the file on the storage will be used to authenticate with Conjur.  The app container will use the authentication file on the shared storage to pull secrets via the API or Summon.
Requirements:
1.	Conjur deployed on the OCP cluster 
1.	This can be a full Conjur stack or just the follower
2.	If using only followers, the follower will need to be seeded from an external Conjur cluster.
3.	Enable Conjur Authenticator 
1.	Used by the Kubenetes API to authenticate the sidecar
4.	Define Conjur Policies to give access to Kubenetes API
5.	Define the sidecar for the openshift app manifest
Deploy Conjur to OCP4:
You can use the following repo to deploy Conjur or Conjur Follower to OCP4.
Configure Conjur:
Conjur will need the following policies configured to allow the Kubenetes API to authenticate with it.
Policies 
Define and upload the following policies:
Policy for human users
At least one user needs write permission to load policy and variables into DAP. This is standard DAP policy that creates an administrative group of users for DAP.
Use the following policy as a template to assign permissions:
# Filename: policy_for_human_users.yml
# initializes users
# ted - kubernetes admin
# bob - devops admin
# alice - db admin
# carol - developer

- !group kube_admin
- !group devops
- !group ops
- !group db_admin

# kube_admin and devops groups are members of the ops admin group
- !grant
  role: !group ops
  members:
  - !group kube_admin
  - !group devops

- !user ted
- !grant
  role: !group kube_admin
  member: !user ted

- !user bob
- !grant
  role: !group devops
  member: !user bob

- !user alice
- !grant
  role: !group db_admin
  member: !user alice

- !user carol
- !grant
  role: !group devops
  member: !user carol

Policy for authentication identities
The identities that will be used to authenticate and retrieve secrets from DAP will also need to be defined in policy and added to the layer that was granted access to the Kubernetes authenticator webservice in the previous policy.
For details about the different identities that can be used in this policy, see Machine Identity
Variable	Description
AUTHENTICATOR_ID	The Conjur authenticator that your Kubernetes or OpenShift applications are using.
TEST_APP_NAMESPACE_NAME	The namespace you are deploying your applications to.
APPLICATION_SERVICE_ACCOUNT	The service account that the application is using.
---
# Filename: policy_for_authenticator_identities.yml
# This policy defines a layer of whitelisted identities permitted to authenticate to the authn-k8s endpoint.
- !policy
  id: conjur/authn-k8s/<AUTHENTICATOR_ID>/apps
  owner: !group devops
  annotations:
    description: Identities permitted to authenticate
  body:
  - !layer
    annotations:
      description: Layer of authenticator identities permitted to call authn svc
  - &hosts
    - !host
      id: <TEST_APP_NAMESPACE_NAME>/*/*
      annotations:
        kubernetes/authentication-container-name: <AUTHENTICATOR_CLIENT_CONTAINER_NAME>
        openshift: "true"
    - !host
      id: <TEST_APP_NAMESPACE_NAME>/service_account/<APPLICATION_SERVICE_ACCOUNT>
      annotations:
        kubernetes/authentication-container-name: <AUTHENTICATOR_CLIENT_CONTAINER_NAME>
        openshift: "true"

  - !grant
    role: !layer
    members: *hosts

Host annotations
Hosts can have these annotations:
•	kubernetes/authentication-container-name: is required to identify the authenticator client. In pre-v5.2.4 deployments, The container name must be the value authenticator. Starting with v5.2.4, there are no restrictions on the authenticator client container name.
•	openshift: true is optional but useful to identify OpenShift hosts in the UI
•	kubernetes: true is optional but useful to identify Kubernetes hosts in the UI
The platform-specific annotations let you see the platform type in the UI and filter hosts by platform.
- &hosts
    - !host
      id: some-namespace-1/*/*
      annotations:
        kubernetes/authentication-container-name: <AUTHENTICATOR_CLIENT_CONTAINTER_NAME>
        openshift: true

Policy for the Kubernetes authenticator service
DAP uses policy to whitelist the applications that have access to the Kubernetes authenticator and store the values necessary to create client certificates for mutual TLS. The authenticator service policy should include:
Resource	Description
Webservice	A resources to represent the authenticator itself
Variable	A resource to hold the CA certificate and key for creating client certificates
Permit statement	Whitelists a layer of application identities
Use the following policy as a template to whitelist applications.
Replace the following variables with values:
Variable	Description
AUTHENTICATOR_ID	The Conjur authenticator that your K8s / OC applications are using.
APPLICATION_NAMESPACE	The namespace you are deploying your applications to.
---
# Filename: policy_for_k8s_authenticator_service.yml
# This policy defines an authn-k8s endpoint, CA creds and a layer for whitelisted identities permitted to authenticate to it
- !policy
  id: conjur/authn-k8s/<AUTHENTICATOR_ID>
  owner: !group devops
  annotations:
    description: Namespace defs for the Conjur cluster in dev
  body:
  - !webservice
    annotations:
      description: authn service for cluster

# CA cert and key for creating client certificates
  - !policy
    id: ca
    body:
    - !variable
      id: cert
      annotations:
        description: CA cert for Kubernetes Pods.
    - !variable
      id: key
      annotations:
        description: CA key for Kubernetes Pods.

  # permit a layer of whitelisted authn ids to call authn service
  - !permit
    resource: !webservice
    privilege: [ read, authenticate ]
    role: !layer /conjur/authn-k8s/<AUTHENTICATOR_ID>/apps

Load policy into DAP
1.	Save policy as .yml files in a location accessible to the DAP Master.
2.	Log into DAP.
3.	Load each policy file:
4.	$ conjur policy load root <policy_for_human_users.yml>
5.	$ conjur policy load root <policy_for_authenticator_identities.yml>
$ conjur policy load root <policy_for_k8s_authenticator_service.yml>
Configure Sidecar:
Configure Sidecar 
Conjur sidecar
This topic describes step-by-step how to enable applications in a Kubernetes environment using a Kubernetes authenticator. It is important to complete each step in the order they are described.
Integrate the Kubernetes authenticator client
To enable an application to authenticate with Conjur, the application manifest must be modified to include:
•	The Kubernetes Authenticator Client as either a sidecar or init container
•	A volume mounted to both containers to share the Conjur access token
•	Application environment variables containing connection information for the Conjur appliance
•	Configure the authenticator client container and add the container name to policy
Create service account for your app
If you are using service accounts to authenticate and you have not created one yet, you will need to do so now. The following example assumes that you are using test-app as the name for your service account.
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-app
Make sure that this service account name matches both policy for authenticator identities in Conjur as well as manifests below wherever < APP_SERVICE_ACCOUNT_NAME > is used.
Add the authenticator image
The Authenticator Client is a public image available on DockerHub . It can be added to the manifest as either a sidecar or init container.
To use the authenticator as a sidecar, insert its image spec alongside the application container image spec. See the Sidecar application manifest example .
To use an init container, insert the authenticator spec under the initContainers property of the pod spec. See the Init container application manifest example . The authenticator client must be configured with the following variables:
Paremeter	Description
CONTAINER_MODE	Specify whether a sidecar or init container is used for the authenticator client. The value is sidecar or init. If this variable is not set, the mode defaults to sidecar.
MY_POD_NAME	This variable identifies the pod where your application runs. Use the Downward API to insert this information into the manifest. In the example manifests, this value is auto-populated fron the app container info.
MY_POD_NAMESPACE	This variable identifies the pod where your application runs. Use the Downward API to insert this information into the manifest. In the example manifests, this value is auto-populated fron the app container info.
MY_POD_IP	This variable identifies the pod where your application runs. Use the Downward API to insert this information into the manifest. In the example manifests, this value is auto-populated from the app container info.
CONJUR_VERSION	The Conjur release number. This currently is either 5 or 4.
CONJUR_AUTHN_URL	Specify the credential service used to log into Conjur. For Conjur v4 use https://conjur-follower.<CONJUR_NAMESPACE_NAME>.svc.cluster.local/api/authn-k8s/<AUTHENTICATOR_ID> and for Conjur v5 use https://conjur-follower.<CONJUR_NAMESPACE_NAME>.svc.cluster.local/authn-k8s/<AUTHENTICATOR_ID>. If you have deployed Conjur OSS version through the Helm chart or GKE marketplace app, change conjur-follower in these examples to the name of the service (e.g. https://conjur-open-source-k8s-conjur-oss.<CONJUR_NAMESPACE_NAME>.svc.cluster.local/authn-k8s/<AUTHENTICATOR_ID>)
CONJUR_ACCOUNT	The account name designated to the Conjur appliance during initial configuration. You most likely set this environment variable before running the deployment scripts. If so, you can use $CONJUR_ACCOUNT for the value here. In most default installations, this account name is default but your configuration may vary.
CONJUR_AUTHN_LOGIN	Specify the Conjur host (Kubernetes resource) that authenticates to Conjur. Set this value to a host id that is defined in policy. See Machine Identity for the host id syntax and the list of Kubernetes resources that can be declared as hosts.
CONJUR_SSL_CERTIFICATE	The public SSL certificate value required for connecting to the Conjur follower service.
The SSL certificate is generated during Conjur appliance configuration and stored in a .pem file located in the root folder where Conjur was created. The file name is conjur-account.pem, where account is the account name provided for the Conjur appliance.
We recommend using a ConfigMap to store the value.
For example, use:
kubectl create configmap conjur-cert --from-file=ssl-certificate="/path/to/ssl/cert"
The equivalent OpenShift command is:
oc create configmap conjur-cert --from-file=ssl-certificate="/path/to/ssl/cert"
The above commands create a ConfigMap that loads the certificate value into it. Now that the certificate value is populated, add the following configuration to the manifest:
configMapKeyRef:
    name: conjur-cert        
    key: ssl_certificate 

Add a shared volume
The authenticator uses a volume shared with the application container to provide a token to authenticate with Conjur and retrieve secrets. The application manifest should define a memory-only volume and then mount it to /run/conjur on both the application and authenticator containers.
Add Conjur connection information
The application must be configured with environment variables that contain connection information for the Conjur appliance. Add the following variables to the application environment:
Paremeter	Description
CONJUR_VERSION	The Conjur release number. This currently is either 5 or 4.
CONJUR_AUTHN_URL	Specify the credential service used to log into Conjur. For Conjur v4 use https://conjur-follower.<CONJUR_NAMESPACE_NAME>.svc.cluster.local/api/authn-k8s/<AUTHENTICATOR_ID> and for Conjur v5 use https://conjur-follower.<CONJUR_NAMESPACE_NAME>.svc.cluster.local/authn-k8s/<AUTHENTICATOR_ID>. If you have deployed Conjur OSS version through the Helm chart or GKE marketplace app, change conjur-follower in these examples to the name of the service (e.g. https://conjur-open-source-k8s-conjur-oss.<CONJUR_NAMESPACE_NAME>.svc.cluster.local/authn-k8s/<AUTHENTICATOR_ID>)
CONJUR_ACCOUNT	The account name designated to the Conjur appliance during initial configuration. You most likely set this environment variable before running the deployment scripts. If so, you can use $CONJUR_ACCOUNT for the value here. In most default installations, this account name is default but your configuration may vary.
CONJUR_AUTHN_TOKEN_FILE	Identifies the complete path and filename where the authentication container should write the Conjur access token that it obtains on behalf of the application.
CONJUR_SSL_CERTIFICATE	The public SSL certificate value required for connecting to the Conjur follower service. We recommend using a ConfigMap to store the value.
The SSL certificate is generated during Conjur appliance configuration and stored in a .pem file located in the root folder where Conjur was created. The file name is conjur-account.pem, where account is the account name provided for the Conjur appliance.
For example, use:
kubectl create configmap conjur-cert --from-file=ssl-certificate="/path/to/ssl/cert"
The equivalent OpenShift command is:
oc create configmap conjur-cert --from-file=ssl-certificate="/path/to/ssl/cert"
The above commands create a ConfigMap that loads the certificate value into it:
configMapKeyRef:
    name: conjur-cert        #ConfigMap name
    key: ssl_certificate  #the key into the ConfigMap

Configure the authenticator client container name
In pre-v5.2.4 deployments, the container name must be the value authenticator. Starting with v5.2.4, there are no restrictions on the authenticator client container name.
Assign the authenticator client container name in the application manifest, in the -image section for the authenticator client. Here is the relevant section in the manifest:
- image: cyberark/conjur-kubernetes-authenticator
        imagePullPolicy: Always
        name: <AUTHENTICATOR_CLIENT_CONTAINER_NAME>
The container name that you assign here must be added into Conjur policy. It must be referenced in the annotation associated with the whitelist of applications that can authenticate to Conjur. See Policy for authentication identities. Here is the relevant section in the policy.
 - !host
    id: <TEST_APP_NAMESPACE_NAME>/*/*
    annotations:
      kubernetes/authentication-container-name: <AUTHENTICATOR_CLIENT_CONTAINER_NAME

Sidecar application manifest example
Init container application manifest example
Define application policy
Use a Conjur policy to define the following:
•	An identity for our application linked to the authentication identities we defined in Policy for authentication identities
•	Variables to hold the secrets for our application that can be written to by at least one human user and read by our application identity

Define policy for application identity
Policy for application secrets
Load policy into Conjur
Prepare applications to retrieve secrets
Use either the Conjur API or Summon to enable applications to retrieve secrets from Conjur.
Summon
Conjur client libraries
Conjur REST API
Load initial secret values into Conjur
For each secret defined in an application policy, load the initial secret value. You can use the Conjur API, the UI, or the CLI for this step.
This is an example using the CLI:
$ conjur variable value cluster-1/db/password abc$xyz
Start the application
To start the application, use this Kubernetes command:
kubectl create -f your-manifest.yaml
The equivalent OpenShift command is:
oc create -f your-manifest.yaml


GPU
Create a machineset as above except specify a GPU instance type. See the section below on installing NFD and SRO.
Install Node Feature Discovery Operator
This operator looks at the hardware features of a node and catalogs them by adding labels to the node. This is how the scheduler is made aware that a node has a GPU.
https://blog.openshift.com/creating-a-gpu-enabled-node-with-openshift-4-2-in-amazon-ec2/
1.	Go to the Operator Hub, search for Node Feature Discovery, and click Install.
2.	Go to Installed Operators and click on Node Feature Discovery.
3.	Click Node Feature Discovery tab and Create Node Feature Discovery. Click Create after the default YAML is displayed
Istio


