The following documentation i sprovided in the https://confluence.capgroup.com/display/CNTEN/vSphere+Install+Steps+OCP+4.x[Capital Group Confluence].

vSphere Install Steps OCP 4.x
•	
•	Pre-Requisites
•	Creating the Ignition (IGN) files
•	Creating VMs
•	Powering Up the VMs
•	Setting Up Local Registry w/ NFS
•	Complete the Rest of the Setup


These instructions were written based on 4.3 install.  Please check with Red Hat documentation for updated instructions if you are using different version of the install. 
There was an extra step in 4.5 install to remove master and worker manifest files that was not part of 4.3. 
https://docs.openshift.com/container-platform/4.5/installing/installing_vsphere/installing-vsphere.html
Pre-Requisites

•	Identified the network address space for the cluster ( preferably anything larger than /27 )
•	Reserve and assign IPs for bootstrap, etcd/master and worker nodes
•	Compute engineering to provide access to vCenter with permissions to the OCP specific folder  ( Point of contact:  RLR )
•	Firewall changes: 
o	Allow OCP network address space to access the following URLs:
Internet Outbound	Whitelisting 
https://cert-api.cloud.redhat.com:443 
https://api.cloud.redhat.com:443 
https://Infogw.api.openshift.com:443 
https://quay.io:443
https://cloud.redhat.com
https://capgroup-dev.oktapreview.com
https://capgroup.okta.com
https://registry.redhat.io
https://registry.svc.ci.openshift.org
https://registry.connect.redhat.com
https://*.cloudfront.net
https://registry-auth.twistlock.com
https://docker.io
https://k8s.gcr.io
https://gcr.io
https://nvcr.io
Access to destination: Internet URL category - "content delivery networks"

Ensure that they also allow it on sjcx-fwb-01-1 	https://cgtstregistry.capgroup.com
https://cgdevregistry.capgroup.com
https://cgprdregistry.capgroup.com
https://cgregistry.capgroup.com
https://syslogvip.capgroup.com:8088
https://ark.capgroup.com
https://jenkins1.capgroup.com
https://jenkins2.capgroup.com






•	F5 
o	Sample template for API Master VIP 
	 
o	Sample template for Apps 
	 
•	DNS 
o	Sample template  
	 
	Validate the DNS
	 

Creating the Ignition (IGN) files
•	For that environment, log onto a RHEL corresponding bastion host.  Existent OCP3.x bastion host should do just fine. 
•	On the bastion host, sudo into the service account.  Create a installation directory named:  ocp4/{ cluster_name } 
•	Log onto https://cloud.redhat.com/openshift/install.  Download both the installer and the CLI tool and untar it onto the bastion host under ocp4/{ cluster_name } 
o	 
•	Create install-config.yaml file 
o	apiVersion: v1
o	baseDomain: capgroup.com
o	compute:
o	- hyperthreading: Enabled
o	  name: worker
o	  replicas: 0
o	controlPlane:
o	  hyperthreading: Enabled
o	  name: master
o	  replicas: 3
o	metadata:
o	  name: <cluster_name>
o	networking:
o	  clusterNetwork:
o	  - cidr: 192.168.0.0/14 
o	    hostPrefix: 23 
o	  networkType: OpenShiftSDN
o	  serviceNetwork: 
o	  - 172.30.0.0/16
o	platform:
o	  vsphere:
o	    vcenter: ''
o	    username: ''
o	    password: ''
o	    datacenter: ''
o	    defaultDatastore: ''
o	pullSecret: '<pull_secret>'
o	sshKey: '<ssh_key>'
•	Copy the pull secret and replace it in the install-config.yaml file { pull_secret }
o	 
•	On the bastion host go to home directory execute "  cat .ssh/id_rsa.pub  ".   Replace the { ssh_key } in  install-config.yaml with the content in id_rsa.pub file. 
o	if ssh key is NOT setup, take the following steps.  Make sure to sudo to service account: 
1.	ssh-keygen -t rsa -b 4096 -N '' -f ~/.ssh/id_rsa
2.	eval "$(ssh-agent -s)"
3.	ssh-add ~/.ssh/id_rsa
•	Backup the install-config.yaml file
•	Run ./openshift-install create manifests --dir=<installation_directory>
•	Modify the manifests/cluster-scheduler-02-config.yml kubernetes manifest file to prevent Pods from being scheduled on the control plane machines:
1.	Open the manifests/cluster-scheduler-02-config.yml file.
2.	Locate the mastersSchedulable parameter and set its value to False.
3.	Save and exit the file.
•	Run ./openshift-install create ignition-configs --dir=<installation_directory>
•	Create append-bootstrap.ign file and back up the file 
o	{   
o	  "ignition": {     
o	    "config": {       
o	      "append": [         
o	        {           
o	          "source": "http://<bastion_FQDN>:8000/bootstrap.ign",       
o	          "verification": {}         
o	        }       
o	      ]     
o	    },     
o	    "timeouts": {},     
o	    "version": "2.1.0"   
o	  },   
o	  "networkd": {},   
o	  "passwd": {},   
o	  "storage": {},   
o	  "systemd": {} 
o	}
•	Run the following commands:  
o	base64 -w0 <installation_directory>/master.ign ><installation_directory>/master.64
base64 -w0<installation_directory>/worker.ign > <installation_directory>/worker.64
base64 -w0 <installation_directory>/append-bootstrap.ign > <installation_directory>/append-bootstrap.64
•	If for any reason you need to re-generate the ignition files.  You will need to delete the following files and directory before running the above commands again. 
o	*.ign
o	*.64
o	.openshift_install_state.json
o	.openshift_install.log
o	auth directory
•	IGNITION FILES WILL NEED TO BE RE-GENERATED IF IT HAVE PASS 24 HOURS AND CLUSTER HAS NOT BEEN SUCCESSFULLY DEPLOYED!!!

Creating VMs
NOTE: These steps are performed by RLR from Compute team . We need to give base 64 files to him requesting him to add them to respective vms as mentioned in step8 while following the steps.
•	Log  ont to vCenter / vSphere Client  ( For login: 2a-account@cguser.capgroup.com )
•	Upload the latest RHCOS OVA vmWare image ( https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/ ) to the folder 
•	Steps to Clone to Virtual Machine ( Need to create 1 bastion, 3 master and 5 worker nodes )
•	
Steps 		
1	Select OVA image.  Right click Clone - Select Clone to Virtual Machine	 
2	Give it a name and select the correct folder to deploy to 
Click NEXT	 
3	Select the proper compute resource option ( Confirm with Compute team for the correct resource to use )
Click NEXT	 
4	Select the proper storage option  ( Confirm with Compute team for the correct resource to use )
Click NEXT	 
5	Select Customize these virtual machine's hardware
Click NEXT	 
6	Enter in the CPU, Memory and storage.

Only for worker nodes:
CPU: 20
Memory: 64Gb
Select Reserve all guest memory ( All locked )
Click on ADD NEW DEVICE button and select Hard Disk
Enter in the desire storage ( These secondary storage block is use for Portworx )

6a. To retain the same MAC address, go to Network Adapter and switch to manual and input or copy the desire MAC address

 
	 
 
 
7	Click on VM Options and select High for Latency Sensitivity 
Click on EDIT CONFIGURATION
Click NEXT	 
8	Click ADD CONFIGURATION PARAMS to create 3 key value

Key	Value
disk.EnableUUID	TRUE
guestinfo.ignition.config.data.encoding	base64
guestinfo.ignition.config.data	( these is the base64 encoded value created as part of Creating the Ignition files ) 

Click OK	 
9	Complete by clicking on FINISH	 
10	Capture the MAC Address .   You will need these to submit a request to Network-NOC for IP reservation mapping to these MAC Address.	 
11	Repeat steps 1 through 10 to create the Masters and Worker nodes	
12	Submit a request to Network-NOC to setup IP Reservation for each of the MAC address.  You will need to provide the IPs and the MAC Address to use.
Ensure the that DHCP options are setup similar to 10.226.4.0/24 network

Validate by checking on IPAM:  
http://sjx1-ipam-1.capgroup.com/incontrol/logon.action
	DHCP Setup ( Point of contact SAMC )
Sample OZ 
 

•	
•	
•	
•	
•	
•	

Powering Up the VMs
•	Before powering up the VMs, you need to do the following:
o	On the Bastion host go to the installation directory and run python -m SimpleHTTPServer 8000  .  These will run a http server listening on port 8000 for use by Bootstrap
•	Now you could power up the VM
•	
Steps		
1	Right click on the Bootstrap VM and click Power On	 
2	Run the following command on the Bastion host:
./openshift-install --dir=<installation directory> wait-for bootstrap-complete --log-level=info	
3	Power up the Master nodes	
4	Power up the Worker nodes	
5	To use the oc commands you need to use export KUBECONFIG	eg: export KUBECONFIG= <directory path of kubeconfig>
export KUBECONFIG=/home/cpzinfbb00000085s002/ocp4/auth/kubeconfig 
6	To see the worker and master nodes getting created after powering up VMs	watch oc get nodes
7	Accept the certificates	oc get csr | while read -r csr _; do oc adm certificate approve "$csr"; done
8	To check the console URL 	oc get routes -n openshift-console
9	After accepting csr logout from KUBECONFIG to avoid using unnecessary oc commands on the cluster.	export KUBECONFIG=
8	IF ANY OF THESE FAILS, YOU WILL NEED TO RE-CREATE THE FAILED VMs!!!	
•	
•	
•	
•	
•	

Setting Up Local Registry w/ NFS
•	
Steps		
1	Open a ticket to request for NAS storage	(  sample request:  REQ0317501 ) .  Make sure to add comment that volume permission set to 775.
2	Log onto the cluster	
3	Edit the registry configuration

Execute:
oc edit configs.imageregistry.operator.openshift.io

Update managementState to Managed 
Under storage key,  add pvc and claim
  storage:
    pvc:
      claim:


	Sample 
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2020-03-18T00:39:12Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 12
  name: cluster
  resourceVersion: "7114851"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 40d26157-0864-493d-a0c6-7c44897eecf6
spec:
  defaultRoute: false
  disableRedirect: false
  httpSecret: b7744adc92ea2886f4850bef7866bd5e955ae448394ff1d8a10445605a96e05c12247401b273a164a85cb7214c279d0013a24904af25f883dbe73c4b941a5374
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    pvc:
      claim:
status:
  conditions:
  - lastTransitionTime: "2020-04-02T21:20:56Z"
    message: The registry is ready
    reason: Ready
    status: "False"
    type: Progressing
  - lastTransitionTime: "2020-04-02T21:20:56Z"
    message: The registry is ready
    reason: Ready
    status: "True"
    type: Available
  - lastTransitionTime: "2020-04-01T20:08:18Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2020-03-31T21:14:32Z"
    status: "False"
    type: Removed
  - lastTransitionTime: "2020-04-02T17:47:04Z"
    reason: PVC Exists
    status: "True"
    type: StorageExists
  observedGeneration: 12
  readyReplicas: 0
  storage:
4	Provision PV
Execute the following command:
oc create -f registry.yaml -n openshift-image-registry	Sample PV yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: image-registry-pv
spec:
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 300Gi
  nfs:
    path: /openShift_oz
    server: n055006.cguser.capgroup.com
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs01
5	Provision PVC

oc create -f registry_pvc.yaml -n openshift-image-registry
	Sample PVC yaml
apiVersion: "v1"
kind: "PersistentVolumeClaim"
metadata:
  name: "image-registry-pvc"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 300Gi
  storageClassName: nfs01
  volumeMode: Filesystem
6	Validate the the content of registry config
	  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    pvc:
      claim: image-registry-pvc
7	Validate by doing a local build	
•	
•	
•	

Complete the Rest of the Setup
•	Use instructions from OCP 4.2 Cluster Provisioning AWS to complete the rest of the setup for the following: 
o	Configuring Wildcard Certificate
o	Okta
o	Portworx
o	Whitelist/ Blacklist Docker Registries
o	Creating default network policies for a new project
o	Splunk
o	Image Pruning
o	Dynatrace
o	AlertManager
o	Add Subscriptions ( Openshift 4 - Cluster Subscription Steps )


