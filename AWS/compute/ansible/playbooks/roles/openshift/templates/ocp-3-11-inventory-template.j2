[OSEv3:children]
masters
etcd
nodes

[OSEv3:vars]

# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_user=ec2-user
ansible_become=yes
#ansible_ssh_common_args='-o StrictHostKeyChecking=no'
ansible_ssh_extra_args='-o StrictHostKeyChecking=no'
# deployment type valid values are origin, online, atomic-enterprise, and openshift-enterprise
openshift_deployment_type=openshift-enterprise

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.11
openshift_clock_enabled=true

#OCP Subscription Credentials
oreg_auth_user=cgocpadmin
oreg_auth_password=
onprem_install=False


# Configure master API and console ports.
openshift_master_api_port=443
openshift_master_console_port=443

#For the lab, disable some pre flight checks.  Disable these in dev and prod.
#openshift_disable_check=disk_availability,memory_availability,package_version
openshift_disable_check=package_version

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
##Pod Network
osm_cluster_network_cidr=192.168.0.0/17

##Service Network
openshift_portal_net=192.168.240.0/20
#osm_host_subnet_length is the length of zeros at the end. Not the number of ones at the start. So 9 equals /23 (32-9)
osm_host_subnet_length=8

# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/ovs-networkpolicy'
os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'

# Enable service catalog
openshift_enable_service_catalog=false
# Enable template service broker (requires service catalog to be enabled, above)
template_service_broker_install=false

# Cloud Provider Configuration
# This section configures the IAM User credentials to allow OpenShift to configure
# dynamic EBS and S3 storage. This can also be done with IAM Instance Profiles,
# but would allow containers running on that host access to these resources. It was
# decided that using API credentials was the preferred method.
# AWS (Using API Credentials)
openshift_cloudprovider_kind=aws
openshift_cloudprovider_aws_access_key={{ openshift_aws_ec2_access_key }}
openshift_cloudprovider_aws_secret_key={{ openshift_aws_ec2_secret_key }}
openshift_clusterid={{ openshift_aws_cluster_name }}

## OpenShift Docker Options
openshift_docker_options='--selinux-enabled --log-opt max-size=1M --log-opt max-file=3'

## htpasswd setup. If not planning on using htpasswd then openshift_master_htpasswd_file can be removed 
# htpasswd-master-record has 2 users/passwords: admin/adm-password and developer/devel-password
openshift_master_htpasswd_file="/home/ec2-user/ocp-provisioning/htpasswd-master-record"

openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'},{'name': 'okta-auth','login': 'true','challenge': 'true','kind': 'OpenIDIdentityProvider','clientID': '0oafv3v9e8QWurhHH0h7','clientSecret': 'AkYfLonMJkjM9P6H4HmKowyQa2yDoBbhtmWH2Dm8','extraScopes': ['email','profile'],'claims': {'id': ['email'],'preferredUsername': ['samAccountName'],'name': ['name'],'email': ['email']},'urls': {'authorize': 'https://capgroup-dev.oktapreview.com/oauth2/ausegqghwnx1MLMC60h7/v1/authorize','token': 'https://capgroup-dev.oktapreview.com/oauth2/ausegqghwnx1MLMC60h7/v1/token'}}]

#openshift_master_overwrite_named_certificates=true
#openshift_master_named_certificates=[{"certfile": "/home/ec2-user/ssl_cert/console_vip/nadqa-west1-console.crt", "keyfile": "/home/ec2-user/ssl_cert/console_vip/nadqa-west1-console.key", "names": ["ocp-console.at-nad-usw1-aws.csp.capgroup.com"]}]
openshift_master_openid_ca_file=/etc/pki/tls/certs/ca-bundle.crt


# VIP setup
# Native high availability cluster method with optional load balancer.
# If no lb group is defined, the installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname={{ openshift_aws_internal_master_vip }}
openshift_master_cluster_public_hostname={{ openshift_aws_master_vip }}

# Following needs wildcard DNS entry to point to Router ELB
# default subdomain to use for exposed routes
openshift_master_default_subdomain={{ openshift_aws_router_vip }}

### OpenShift Registry deployment with S3
# Since the registry prefers object storage, S3 was configured for optimal performance.
openshift_hosted_registry_storage_kind=object
openshift_hosted_registry_storage_provider=s3
openshift_hosted_registry_storage_s3_encrypt=true
openshift_hosted_registry_storage_s3_accesskey={{ openshift_aws_s3_access_key }}
openshift_hosted_registry_storage_s3_secretkey={{ openshift_aws_s3_secret_key }}
openshift_hosted_registry_storage_s3_bucket={{ openshift_aws_s3_bucket_name }}
openshift_hosted_registry_storage_s3_region={{ openshift_aws_s3_bucket_region }}
openshift_hosted_registry_storage_s3_chunksize=26214400
openshift_hosted_registry_storage_s3_rootdirectory=/registry
openshift_hosted_registry_pullthrough=true
openshift_hosted_registry_acceptschema2=true
openshift_hosted_registry_enforcequota=true
openshift_hosted_registry_selector='node-role.kubernetes.io/infra=true'
openshift_hosted_registry_replicas=3

#### router
openshift_hosted_router_selector='node-role.kubernetes.io/infra=true'
openshift_hosted_router_replicas=3
# If not using a custom wildcard certificate then comment out openshift_hosted_router_certificate 
#openshift_hosted_router_certificate={"certfile": "/home/ec2-user/ssl_cert/wildcard_vip/nadqa-west1-wildcard.crt", "keyfile": "/home/ec2-user/ssl_cert/wildcard_vip/nadqa-west1-wildcard.key", "cafile": "/home/ec2-user/ssl_cert/wildcard_vip/nadqa-west1-wildcard.ca"}
openshift_additional_ca=/home/ozinfbb00000085s002/ssl_cert/cg_ca.crt

### OpenShift EFK stack deployment
# Since EBS is an excellent alternative to NFS, we configure the EFK stack with
# persistent storage here.
openshift_logging_install_logging=false
openshift_logging_kibana_hostname=kibana.{{ openshift_aws_router_vip }}
openshift_logging_kibana_ops_hostname=kibana.{{ openshift_aws_router_vip }}
openshift_master_logging_public_url=https://kibana.{{ openshift_aws_router_vip }}
openshift_logging_master_public_url=https://{{ openshift_aws_master_vip }}
openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_storage_class_name=aws-ebs-logging
openshift_logging_es_pvc_size=100Gi
openshift_logging_curator_default_days=15
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra":"true",'label': 'infra-logging'}
openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra":"true"}

### OpenShift Metrics (Hawkular)
openshift_metrics_install_metrics=false
openshift_metrics_hawkular_hostname=hawkular-metrics.{{ openshift_aws_router_vip }}
openshift_master_metrics_public_url=https://hawkular-metrics.{{ openshift_aws_router_vip }}/hawkular/metrics
openshift_metrics_cassandra_storage_type=dynamic
openshift_metrics_storage_volume_size=20Gi
openshift_metrics_hawkular_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_metrics_cassandra_nodeselector={"node-role.kubernetes.io/infra":"true",'label': 'infra-cassandra'}
openshift_metrics_heapster_nodeselector={"node-role.kubernetes.io/infra":"true"}

### Node selector for apps
osm_default_node_selector='node-role.kubernetes.io/compute=true'

### Post install variables
# Setup StorageClass EBS Zone for logging and metrics
logging_zone={{ openshift_aws_logging_ebs_zone }}
metrics_zone={{ openshift_aws_metric_ebs_zone }}
volume_encryption_arn= {{ aws_volume_encryption_arn }}
# Set MTU for IPSEC. Verify that this value is valid for target environment.
# Comment out if IPSec is to not going to be installed. 
node_sdn_mtu=8889
# Docker service value
default_registry=docker-registry.default.svc:5000
# Logging operations projects
logging_ops_projects=['default', 'openshift', 'openshift-infra', 'kube-system']
# Are the cluster nodes domained joined?
domain_joined=False

#Config Map
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true'], 'edits': [{'key': 'kubeletArguments.kube-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.system-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<500Mi']},{'key': 'kubeletArguments.max-pods', 'value': ['250']},{'key': 'kubeletArguments.pods-per-core','value': ['10']},{'key': 'kubeletArguments.maximum-dead-containers','value': ['5']},{'key': 'kubeletArguments.maximum-dead-containers-per-container','value': ['1']},{'key': 'kubeletArguments.image-gc-high-threshold','value': ['80']},{'key': 'kubeletArguments.image-gc-low-threshold','value': ['60']} ]}, {'name': 'node-config-infra1', 'labels': ['node-role.kubernetes.io/infra=true','label=infra-logging'], 'edits': [{'key': 'kubeletArguments.kube-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.system-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<500Mi']},{'key': 'kubeletArguments.max-pods', 'value': ['250']},{'key': 'kubeletArguments.pods-per-core','value': ['10']},{'key': 'kubeletArguments.maximum-dead-containers','value': ['5']},{'key': 'kubeletArguments.maximum-dead-containers-per-container','value': ['1']},{'key': 'kubeletArguments.image-gc-high-threshold','value': ['80']},{'key': 'kubeletArguments.image-gc-low-threshold','value': ['60']}]}, {'name': 'node-config-infra2', 'labels': ['node-role.kubernetes.io/infra=true', 'label=infra-cassandra'], 'edits': [{'key': 'kubeletArguments.kube-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.system-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<500Mi']},{'key': 'kubeletArguments.max-pods', 'value': ['250']},{'key': 'kubeletArguments.pods-per-core','value': ['10']},{'key': 'kubeletArguments.maximum-dead-containers','value': ['5']},{'key': 'kubeletArguments.maximum-dead-containers-per-container','value': ['1']},{'key': 'kubeletArguments.image-gc-high-threshold','value': ['80']},{'key': 'kubeletArguments.image-gc-low-threshold','value': ['60']}]}, {'name': 'node-config-infra3', 'labels': ['node-role.kubernetes.io/infra=true'], 'edits': [{'key': 'kubeletArguments.kube-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.system-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<500Mi']},{'key': 'kubeletArguments.max-pods', 'value': ['250']},{'key': 'kubeletArguments.pods-per-core','value': ['10']},{'key': 'kubeletArguments.maximum-dead-containers','value': ['5']},{'key': 'kubeletArguments.maximum-dead-containers-per-container','value': ['1']},{'key': 'kubeletArguments.image-gc-high-threshold','value': ['80']},{'key': 'kubeletArguments.image-gc-low-threshold','value': ['60']}]}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true','region=primary'], 'edits': [{'key': 'kubeletArguments.kube-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.system-reserved', 'value': ['cpu=1,memory=2Gi']},{'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<500Mi']},{'key': 'kubeletArguments.max-pods', 'value': ['250']},{'key': 'kubeletArguments.pods-per-core','value': ['10']},{'key': 'kubeletArguments.maximum-dead-containers','value': ['5']},{'key': 'kubeletArguments.maximum-dead-containers-per-container','value': ['1']},{'key': 'kubeletArguments.image-gc-high-threshold','value': ['80']},{'key': 'kubeletArguments.image-gc-low-threshold','value': ['60']}]}]

#Auditing
#openshift_master_audit_config={"enabled": true, "policyFile": "/etc/origin/master/audit-config.yaml", "auditFilePath": "/var/log/audit-ocp.log", "maximumFileRetentionDays": 10, "maximumFileSizeMegabytes": 50, "maximumRetainedFiles": 5, "logFormat": "legacy", "policyFile": "/etc/origin/master/audit-config.yaml"}

# host group for masters
[masters]
{{ openshift_aws_master1_private_hostname }} 
{{ openshift_aws_master2_private_hostname }} 
{{ openshift_aws_master3_private_hostname }} 

# host group for etcd
[etcd]
{{ openshift_aws_master1_private_hostname }} 
{{ openshift_aws_master2_private_hostname }} 
{{ openshift_aws_master3_private_hostname }} 

# host group for infra nodes
# This section isnt required but gives an easy way to send ad hoc ansible commands
# to just the infra hosts.
[infra]
{{ openshift_aws_infra1_private_hostname }}
{{ openshift_aws_infra2_private_hostname }}
{{ openshift_aws_infra3_private_hostname }}

# host group for app nodes
# This section isnt required but gives an easy way to send ad hoc ansible commands
# to just the app hosts.
[appnodes]
{{ openshift_aws_app1_private_hostname }}
{{ openshift_aws_app2_private_hostname }}
{{ openshift_aws_app3_private_hostname }}

[nodes]
{{ openshift_aws_master1_private_hostname }}  openshift_node_group_name='node-config-master'
{{ openshift_aws_master2_private_hostname }}  openshift_node_group_name='node-config-master'
{{ openshift_aws_master3_private_hostname }}  openshift_node_group_name='node-config-master' 

{{ openshift_aws_infra1_private_hostname }} openshift_node_group_name='node-config-infra1'
{{ openshift_aws_infra2_private_hostname }} openshift_node_group_name='node-config-infra2'
{{ openshift_aws_infra3_private_hostname }} openshift_node_group_name='node-config-infra3'

{{ openshift_aws_app1_private_hostname }} openshift_node_group_name='node-config-compute'
{{ openshift_aws_app2_private_hostname }} openshift_node_group_name='node-config-compute'
{{ openshift_aws_app3_private_hostname }} openshift_node_group_name='node-config-compute'

