---
  
##### 
# These playbook is for configuring IPSEC on Openshift Cloud Platform
# It should only meant to run as post install of OCP is completed and on control/bastion host
# To run:  ansible-playbook -i < inventory cfg file > ocp_ipsec_playbook.yml
# 
#  Remember to replace <ENV_VAR_FILE> with the appropriate env variable file under vars/
#
#  LastUpdated: 2019-01-18
#  UpdatedBy: Philip Phan ( phlp@capgroup.com )
#

- hosts: nodes
  gather_facts: false
  tasks:
  #- name: set global variable file to use throughout the playbook
    #set_fact: 
      #openshift_aws_env_var_file: <ENV_VAR_FILE> 
  - name: include env file to work around ansible 2.4 issue
    include_vars: roles/aws_openshift3-11_ipsec/vars/{{ env_var_file }}


- hosts: masters[0]
  gather_facts: false
  tasks:
  - name: Generate a new cluster cert and key on a master
    shell:
      oc adm ca create-signer-cert --name=ipsec-signer --cert=/home/{{ansible_user}}/ipsec2/ca.crt --key=/home/{{ansible_user}}/ipsec2/ca.key --serial=/home/{{ansible_user}}/ipsec2/ca.serial.txt

  - name: Generate certs for each node in the cluster
    shell:
      oc adm ca create-server-cert --signer-cert=/home/{{ansible_user}}/ipsec2/ca.crt --signer-key=/home/{{ansible_user}}/ipsec2/ca.key --signer-serial=/home/{{ansible_user}}/ipsec2/ca.serial.txt --cert=/home/{{ansible_user}}/ipsec2/servers/{{ item }}.crt --key=/home/{{ansible_user}}/ipsec2/servers/{{ item }}.key --hostnames={{ item }}
    with_items: "{{ groups['nodes'] }}"

  - name: Retrieve ca crt file from master
    fetch:
      src: ipsec2/ca.crt
      dest: /home/{{ansible_user}}/ipsec_certs/
      flat: yes
      fail_on_missing: yes

  - name: Fetch node certificate from master to bastion 
    fetch:
      src: ipsec2/servers/{{ item }}.crt
      dest: /home/{{ansible_user}}/ipsec_certs/
      flat: yes
      fail_on_missing: yes
    with_items: "{{ groups['nodes'] }}"

  - name: Fetch node key from master to bastion
    fetch:
      src: ipsec2/servers/{{ item }}.key
      dest: /home/{{ansible_user}}/ipsec_certs/
      flat: yes
      fail_on_missing: yes
    with_items: "{{ groups['nodes'] }}"

  - name: Retrieve all config maps in openshift-node namespace 
    shell: oc get cm -n openshift-node | awk '{print $1}' | sed 1d
    register: openshift_node_config_maps

  - name: Output all config maps in openshift-node namespace
    debug:
      msg: "{{ item }}"
    with_items:  "{{ openshift_node_config_maps.stdout_lines }}"

  - name: export config map files before making changes
    shell: oc get cm {{ item }} -n openshift-node -o yaml > /tmp/{{ item }}.yaml
    with_items:  "{{ openshift_node_config_maps.stdout_lines }}"

  - name: get timestamp from the system
    shell: "date +%Y-%m-%d%H-%M-%S"
    register: tstamp

  - name: backup the config map files before making changes
    become: yes
    copy:
      src: /tmp/{{ item }}.yaml
      dest: /tmp/{{ item }}.yaml.{{ tstamp.stdout[10:] }}
      remote_src: yes
      backup: yes  
    with_items:  "{{ openshift_node_config_maps.stdout_lines }}"
  
  - name: Change mtu value to 8889 in config maps 
    replace:
      path: /tmp/{{ item }}.yaml
      regexp: 'mtu: 8951'
      replace: 'mtu: 8889'
    with_items:  "{{ openshift_node_config_maps.stdout_lines }}"

  - name: please review the config map located in /tmp directory on master node and confirm to continue 
    pause: prompt='Please review. Press return to continue to update node configuration. Press Ctrl+c and then "a" to abort'

  - name: updating mtu value in config map 
    shell: oc replace -f /tmp/{{ item }}.yaml -n openshift-node 
    with_items:  "{{ openshift_node_config_maps.stdout_lines }}"

  # - name: delete ovs and sdn pods
  #   shell: oc delete pod -l app={{ item }} -n openshift-sdn
  #   with_items: 
  #     - sdn
  #     - ovs

  - name: delete ovs and sdn pods
    shell: oc delete pod -l app={{ item }} -n openshift-sdn
    loop: 
      - sdn
      - ovs
      - sdn
      - ovs
    loop_control:
      pause: 3

  - name: please review the mtu value of all nodes - 'ansible nodes -i <inventory> -a "ip addr show br0"'
    pause: prompt='MTU value should be 8889. If not, run "oc delete pod -l app=sdn -n openshift-sdn; oc delete pod -l app=ovs -n openshift-sdn" Press return to continue  Press Ctrl+c and then "a" to abort'

- hosts: nodes
  tasks:

  - name: Copy over generated cert files to all nodes
    copy:
      src: /home/{{ansible_user}}/ipsec_certs/
      dest: /etc/origin/node/generated_certs/
      owner: root
      group: root
      mode: u=rw,g=r,o=r

  - name: yum install libreswan
    yum:
      name: libreswan
      state: latest
  
  - name: check if nss cert database exists
    stat:
      path: /etc/ipsec.d/cert9.db
    register: nss_cert_exists

  - name: remove cert9.db database if it already exists
    file:
      path: /etc/ipsec.d/cert9.db
      state: absent
    when: nss_cert_exists.stat.exists == True
    
  - name: check if nss key database exists
    stat:
      path: /etc/ipsec.d/key4.db
    register: nss_key_exists

  - name: remove key4.db if it already exists 
    file:
      path: /etc/ipsec.d/key4.db
      state: absent
    when: nss_key_exists.stat.exists == True

  - name: initialize NSS database 
    shell: ipsec initnss

  # pause for 10 secs before proceeding
  - name: pause a bit before continuing
    pause:
      seconds: 10

  - name: combine node certs into a PKCS12 file
    shell: 'openssl pkcs12 -export -in /etc/origin/node/generated_certs/{{ ansible_fqdn }}.crt -inkey /etc/origin/node/generated_certs/{{ ansible_fqdn }}.key -certfile /etc/origin/node/generated_certs/ca.crt -passout pass: -out /tmp/mypkcs'
  
  - name: Load the generated PKCS#12 file into the NSS database
    shell: 'pk12util -i /tmp/mypkcs -d sql:/etc/ipsec.d -W ""'

  - name: delete the generated PKCS12 file
    file: path='/tmp/mypkcs' state=absent

- hosts: nodes
  tasks:
  - name: create openshift cluster config /etc/ipsec.d/openshift-cluster.conf
    include_role:
      name: aws_openshift3-11_ipsec
      #vars_from: "{{ openshift_aws_env_var_file }}"
      tasks_from: ipsec_create_copy_cluster_conf.yml
    when: openshift_create_and_copy_cluster_conf

- hosts: localhost
  connection: local
  tasks:
  - name: include env file to work around ansible 2.4 issue
    include_vars: roles/aws_openshift3-11_ipsec/vars/{{ env_var_file }}
  - name: create private and clear policies 
    include_role:
      name: aws_openshift3-11_ipsec
      #vars_from: "{{ openshift_aws_env_var_file }}"
      tasks_from: ipsec_create_policies.yml
    when: openshift_create_policies

- hosts: nodes
  tasks:
  - name: copy private and clear policies to /etc/ipsec.d/ directory
    include_role:
      name: aws_openshift3-11_ipsec
      #vars_from: "{{ openshift_aws_env_var_file }}"
      tasks_from: ipsec_copy_policies.yml
    when: openshift_copy_policies

  - name: configure iptables
    include_role:
      name: aws_openshift3-11_ipsec
      #vars_from: "{{ openshift_aws_env_var_file }}"
      tasks_from: ipsec_configure_iptables.yml
    when: openshift_configure_iptables 

- hosts: nodes
  tasks:
  - name: prompt for user whether to continue or not
    pause: prompt='Please confirm you want to start IPSec! Press return to continue. Press Ctrl+c and then "a" to abort'

  - name: "Start ipsec service" 
    service: 
      name: ipsec
      state: started

  # pause for 10 secs before proceeding
  - name: pause for 10 seconds
    pause:
      seconds: 10

- hosts: masters[0]
  gather_facts: false
  tasks:
  - name: verify cluster is communicating properly
    shell: 'oc get pods --all-namespaces'  

- hosts: nodes
  gather_facts: false
  tasks:
  - name: prompt for user whether to continue or not
    pause: prompt='Please confirm you want to enable IPSec! Press return to continue. Press Ctrl+c and then "a" to abort'


  - name: "Enable ipsec service" 
    service: 
      name: ipsec
      state: started
      enabled: yes

